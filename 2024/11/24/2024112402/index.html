<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Transformer示例代码解读 | SEER's Study</title><meta name="author" content="SEER"><meta name="copyright" content="SEER"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="window.MathJax &#x3D; {     tex: {       inlineMath: [[&#39;$&#39;, &#39;$&#39;], [&#39;\\(&#39;, &#39;\\)&#39;]],         displayMath: [[&#39;$$&#39;, &#39;$$&#39;], [&#39;\\[&#39;, &#39;\\]&#39;]]       }   };    1.代码展示Transformer示例代码如下。 12345678910111213141516171">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer示例代码解读">
<meta property="og:url" content="https://seer666.github.io/2024/11/24/2024112402/index.html">
<meta property="og:site_name" content="SEER&#39;s Study">
<meta property="og:description" content="window.MathJax &#x3D; {     tex: {       inlineMath: [[&#39;$&#39;, &#39;$&#39;], [&#39;\\(&#39;, &#39;\\)&#39;]],         displayMath: [[&#39;$$&#39;, &#39;$$&#39;], [&#39;\\[&#39;, &#39;\\]&#39;]]       }   };    1.代码展示Transformer示例代码如下。 12345678910111213141516171">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg">
<meta property="article:published_time" content="2024-11-24T06:30:00.000Z">
<meta property="article:modified_time" content="2024-11-24T08:49:44.908Z">
<meta property="article:author" content="SEER">
<meta property="article:tag" content="代码">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><link rel="shortcut icon" href="https://avatars.githubusercontent.com/u/126209991?v=4"><link rel="canonical" href="https://seer666.github.io/2024/11/24/2024112402/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.1.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.6.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        if (name && globalFn[key][name]) return
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Transformer示例代码解读',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-11-24 16:49:44'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><a href="https://github.com/SEER666" target="_blank"> <img src="https://avatars.githubusercontent.com/u/126209991?v=4" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></a></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">40</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 列表</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我们</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="https://avatars.githubusercontent.com/u/126209991?v=4" alt="Logo"><span class="site-name">SEER's Study</span></a><a class="nav-page-title" href="/"><span class="site-name">Transformer示例代码解读</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 列表</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我们</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Transformer示例代码解读</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-11-24T06:30:00.000Z" title="发表于 2024-11-24 14:30:00">2024-11-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-11-24T08:49:44.908Z" title="更新于 2024-11-24 16:49:44">2024-11-24</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],  
      displayMath: [['$$', '$$'], ['\\[', '\\]']]  
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<h2 id="1-代码展示"><a href="#1-代码展示" class="headerlink" title="1.代码展示"></a>1.代码展示</h2><p>Transformer示例代码如下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, Tensor</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> TransformerEncoder, TransformerEncoderLayer</span><br><span class="line"><span class="keyword">from</span> torchtext.data.utils <span class="keyword">import</span> get_tokenizer</span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> build_vocab_from_iterator</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> dataset</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> tempfile <span class="keyword">import</span> TemporaryDirectory</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Tuple</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义Transformer模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ntoken: <span class="built_in">int</span>, d_model: <span class="built_in">int</span>, nhead: <span class="built_in">int</span>, d_hid: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 nlayers: <span class="built_in">int</span>, dropout: <span class="built_in">float</span> = <span class="number">0.5</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.model_type = <span class="string">&#x27;Transformer&#x27;</span></span><br><span class="line">        <span class="variable language_">self</span>.pos_encoder = PositionalEncoding(d_model, dropout)</span><br><span class="line">        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)</span><br><span class="line">        <span class="variable language_">self</span>.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)</span><br><span class="line">        <span class="variable language_">self</span>.encoder = nn.Embedding(ntoken, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.d_model = d_model</span><br><span class="line">        <span class="variable language_">self</span>.decoder = nn.Linear(d_model, ntoken)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        initrange = <span class="number">0.1</span></span><br><span class="line">        <span class="variable language_">self</span>.encoder.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        <span class="variable language_">self</span>.decoder.bias.data.zero_()</span><br><span class="line">        <span class="variable language_">self</span>.decoder.weight.data.uniform_(-initrange, initrange)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src: Tensor, src_mask: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        src = <span class="variable language_">self</span>.encoder(src) * math.sqrt(<span class="variable language_">self</span>.d_model)</span><br><span class="line">        src = <span class="variable language_">self</span>.pos_encoder(src)</span><br><span class="line">        output = <span class="variable language_">self</span>.transformer_encoder(src, src_mask)</span><br><span class="line">        output = <span class="variable language_">self</span>.decoder(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义位置编码</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, dropout: <span class="built_in">float</span> = <span class="number">0.1</span>, max_len: <span class="built_in">int</span> = <span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        position = torch.arange(max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe = torch.zeros(max_len, <span class="number">1</span>, d_model)</span><br><span class="line">        pe[:, <span class="number">0</span>, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">0</span>, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        x = x + <span class="variable language_">self</span>.pe[:x.size(<span class="number">0</span>)]</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.dropout(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_square_subsequent_mask</span>(<span class="params">sz: <span class="built_in">int</span></span>) -&gt; Tensor:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Generates an upper-triangular matrix of -inf, with zeros on diag.&quot;&quot;&quot;</span></span><br><span class="line">    mask = torch.triu(torch.ones(sz, sz) * <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>), diagonal=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> mask</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载本地Wikitext-2数据集并预处理</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_and_preprocess_data</span>(<span class="params">local_data_path: <span class="built_in">str</span></span>):</span><br><span class="line">    <span class="comment"># 验证数据路径是否存在</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(local_data_path):</span><br><span class="line">        <span class="keyword">raise</span> FileNotFoundError(<span class="string">f&quot;Dataset not found at <span class="subst">&#123;local_data_path&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用torchtext加载本地数据</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">yield_tokens</span>(<span class="params">file_path</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(file_path, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                <span class="keyword">yield</span> tokenizer(line.strip())</span><br><span class="line"></span><br><span class="line">    tokenizer = get_tokenizer(<span class="string">&#x27;basic_english&#x27;</span>)</span><br><span class="line">    vocab = build_vocab_from_iterator(yield_tokens(os.path.join(local_data_path, <span class="string">&#x27;wiki.train.tokens&#x27;</span>)), specials=[<span class="string">&#x27;&lt;unk&gt;&#x27;</span>])</span><br><span class="line">    vocab.set_default_index(vocab[<span class="string">&#x27;&lt;unk&gt;&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">data_process</span>(<span class="params">file_path</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(file_path, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">return</span> torch.cat([torch.tensor(vocab(tokenizer(line.strip())), dtype=torch.long) <span class="keyword">for</span> line <span class="keyword">in</span> f])</span><br><span class="line"></span><br><span class="line">    train_data = data_process(os.path.join(local_data_path, <span class="string">&#x27;wiki.train.tokens&#x27;</span>))</span><br><span class="line">    val_data = data_process(os.path.join(local_data_path, <span class="string">&#x27;wiki.valid.tokens&#x27;</span>))</span><br><span class="line">    test_data = data_process(os.path.join(local_data_path, <span class="string">&#x27;wiki.test.tokens&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_data, val_data, test_data, vocab</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据批处理函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batchify</span>(<span class="params">data: Tensor, bsz: <span class="built_in">int</span></span>) -&gt; Tensor:</span><br><span class="line">    seq_len = data.size(<span class="number">0</span>) // bsz</span><br><span class="line">    data = data[:seq_len * bsz]</span><br><span class="line">    data = data.view(bsz, seq_len).t().contiguous()</span><br><span class="line">    <span class="keyword">return</span> data.to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据加载与处理</span></span><br><span class="line">local_data_path = <span class="string">&quot;wikitext-2&quot;</span>  <span class="comment"># 替换为本地Wikitext-2数据集路径</span></span><br><span class="line">train_data, val_data, test_data, vocab = load_and_preprocess_data(local_data_path)</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">batch_size = <span class="number">20</span></span><br><span class="line">eval_batch_size = <span class="number">10</span></span><br><span class="line">train_data = batchify(train_data, batch_size)</span><br><span class="line">val_data = batchify(val_data, eval_batch_size)</span><br><span class="line">test_data = batchify(test_data, eval_batch_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型与训练配置</span></span><br><span class="line">ntokens = <span class="built_in">len</span>(vocab)</span><br><span class="line">emsize = <span class="number">200</span></span><br><span class="line">d_hid = <span class="number">200</span></span><br><span class="line">nlayers = <span class="number">2</span></span><br><span class="line">nhead = <span class="number">2</span></span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line">model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">lr = <span class="number">5.0</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=lr)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optimizer, <span class="number">1.0</span>, gamma=<span class="number">0.95</span>)</span><br><span class="line"></span><br><span class="line">bptt = <span class="number">35</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练和评估函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model: nn.Module</span>):</span><br><span class="line">    model.train()</span><br><span class="line">    total_loss = <span class="number">0.</span></span><br><span class="line">    log_interval = <span class="number">200</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    src_mask = generate_square_subsequent_mask(bptt).to(device)</span><br><span class="line"></span><br><span class="line">    num_batches = <span class="built_in">len</span>(train_data) // bptt</span><br><span class="line">    <span class="keyword">for</span> batch, i <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">range</span>(<span class="number">0</span>, train_data.size(<span class="number">0</span>) - <span class="number">1</span>, bptt)):</span><br><span class="line">        data, targets = get_batch(train_data, i)</span><br><span class="line">        seq_len = data.size(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> seq_len != bptt:</span><br><span class="line">            src_mask = src_mask[:seq_len, :seq_len]</span><br><span class="line">        output = model(data, src_mask)</span><br><span class="line">        loss = criterion(output.view(-<span class="number">1</span>, ntokens), targets)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">0.5</span>)</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> batch % log_interval == <span class="number">0</span> <span class="keyword">and</span> batch &gt; <span class="number">0</span>:</span><br><span class="line">            lr = scheduler.get_last_lr()[<span class="number">0</span>]</span><br><span class="line">            ms_per_batch = (time.time() - start_time) * <span class="number">1000</span> / log_interval</span><br><span class="line">            cur_loss = total_loss / log_interval</span><br><span class="line">            ppl = math.exp(cur_loss)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;| epoch <span class="subst">&#123;epoch:3d&#125;</span> | <span class="subst">&#123;batch:5d&#125;</span>/<span class="subst">&#123;num_batches:5d&#125;</span> batches | &#x27;</span></span><br><span class="line">                  <span class="string">f&#x27;lr <span class="subst">&#123;lr:<span class="number">02.2</span>f&#125;</span> | ms/batch <span class="subst">&#123;ms_per_batch:<span class="number">5.2</span>f&#125;</span> | &#x27;</span></span><br><span class="line">                  <span class="string">f&#x27;loss <span class="subst">&#123;cur_loss:<span class="number">5.2</span>f&#125;</span> | ppl <span class="subst">&#123;ppl:<span class="number">8.2</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">            total_loss = <span class="number">0</span></span><br><span class="line">            start_time = time.time()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">model: nn.Module, eval_data: Tensor</span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    total_loss = <span class="number">0.</span></span><br><span class="line">    src_mask = generate_square_subsequent_mask(bptt).to(device)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, eval_data.size(<span class="number">0</span>) - <span class="number">1</span>, bptt):</span><br><span class="line">            data, targets = get_batch(eval_data, i)</span><br><span class="line">            seq_len = data.size(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">if</span> seq_len != bptt:</span><br><span class="line">                src_mask = src_mask[:seq_len, :seq_len]</span><br><span class="line">            output = model(data, src_mask)</span><br><span class="line">            output_flat = output.view(-<span class="number">1</span>, ntokens)</span><br><span class="line">            total_loss += seq_len * criterion(output_flat, targets).item()</span><br><span class="line">    <span class="keyword">return</span> total_loss / (<span class="built_in">len</span>(eval_data) - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_batch</span>(<span class="params">source: Tensor, i: <span class="built_in">int</span></span>) -&gt; <span class="type">Tuple</span>[Tensor, Tensor]:</span><br><span class="line">    seq_len = <span class="built_in">min</span>(bptt, <span class="built_in">len</span>(source) - <span class="number">1</span> - i)</span><br><span class="line">    data = source[i:i+seq_len]</span><br><span class="line">    target = source[i+<span class="number">1</span>:i+<span class="number">1</span>+seq_len].reshape(-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> data, target</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练循环</span></span><br><span class="line">best_val_loss = <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)</span><br><span class="line">epochs = <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> TemporaryDirectory() <span class="keyword">as</span> tempdir:</span><br><span class="line">    best_model_params_path = os.path.join(tempdir, <span class="string">&quot;best_model_params.pt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">        epoch_start_time = time.time()</span><br><span class="line">        train(model)</span><br><span class="line">        val_loss = evaluate(model, val_data)</span><br><span class="line">        val_ppl = math.exp(val_loss)</span><br><span class="line">        elapsed = time.time() - epoch_start_time</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">89</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;| end of epoch <span class="subst">&#123;epoch:3d&#125;</span> | time: <span class="subst">&#123;elapsed:<span class="number">5.2</span>f&#125;</span>s | &#x27;</span></span><br><span class="line">              <span class="string">f&#x27;valid loss <span class="subst">&#123;val_loss:<span class="number">5.2</span>f&#125;</span> | valid ppl <span class="subst">&#123;val_ppl:<span class="number">8.2</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">89</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> val_loss &lt; best_val_loss:</span><br><span class="line">            best_val_loss = val_loss</span><br><span class="line">            torch.save(model.state_dict(), best_model_params_path)</span><br><span class="line"></span><br><span class="line">        scheduler.step()</span><br><span class="line"></span><br><span class="line">    model.load_state_dict(torch.load(best_model_params_path))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试模型</span></span><br><span class="line">test_loss = evaluate(model, test_data)</span><br><span class="line">test_ppl = math.exp(test_loss)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;=&#x27;</span> * <span class="number">89</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;| End of training | test loss <span class="subst">&#123;test_loss:<span class="number">5.2</span>f&#125;</span> | test ppl <span class="subst">&#123;test_ppl:<span class="number">8.2</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;=&#x27;</span> * <span class="number">89</span>)</span><br></pre></td></tr></table></figure>

<h2 id="2-数据集下载"><a href="#2-数据集下载" class="headerlink" title="2.数据集下载"></a>2.数据集下载</h2><p>数据集下载地址如下：<a target="_blank" rel="noopener" href="https://blog.csdn.net/gitblog_09755/article/details/142889183#:~:text=%E6%9C%AC%E4%BB%93%E5%BA%93%E6%8F%90%E4%BE%9B%E4%BA%86%E4%B8%80%E4%BB%BDWikitext-2-v1%E7%9A%84%E6%A0%87%E5%87%86%E6%95%B0%E6%8D%AE%E5%8C%85%EF%BC%8C%E6%96%B9%E4%BE%BF%E6%97%A0%E6%B3%95%E9%80%9A%E8%BF%87%E4%BA%9A%E9%A9%AC%E9%80%8A%E7%BD%91%E5%9D%80%E4%B8%8B%E8%BD%BD%E7%9A%84%E7%94%A8%E6%88%B7%E8%8E%B7%E5%8F%96%E3%80%82%20%E5%8E%8B%E7%BC%A9%E5%8C%85%E5%86%85%E5%8C%85%E5%90%AB%E4%BB%A5%E4%B8%8B%E6%96%87%E4%BB%B6%EF%BC%9A%20%E8%BF%99%E4%BA%9B%E6%96%87%E4%BB%B6%E6%98%AFWikitext-2-v1%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E6%A0%B8%E5%BF%83%E7%BB%84%E6%88%90%E9%83%A8%E5%88%86%EF%BC%8C%E9%80%82%E7%94%A8%E4%BA%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E7%9A%84%E7%A0%94%E7%A9%B6%E5%92%8C%E5%AE%9E%E9%AA%8C%E3%80%82%20%E4%B8%8B%E8%BD%BD%E6%9C%AC%E4%BB%93%E5%BA%93%E6%8F%90%E4%BE%9B%E7%9A%84%E5%8E%8B%E7%BC%A9%E5%8C%85%E3%80%82%20%E8%A7%A3%E5%8E%8B%E5%90%8E%E5%8D%B3%E5%8F%AF%E8%8E%B7%E5%BE%97%E6%89%80%E9%9C%80%E7%9A%84,wiki.test.tokens%20%E3%80%81%20wiki.train.tokens%20%E5%92%8C%20wiki.valid.tokens%20%E6%96%87%E4%BB%B6%E3%80%82">wikitext-2 CSDN链接</a></p>
<p>下载后，数据集与py文件的部署方式如下：</p>
<ul>
<li>project&#x2F; <ul>
<li>wikitext-2<ul>
<li>wiki.test.token </li>
<li>wiki.train.token </li>
<li>wiki.valid.token</li>
</ul>
</li>
<li>transformer_tutorial.py</li>
</ul>
</li>
</ul>
<h2 id="3-代码解读"><a href="#3-代码解读" class="headerlink" title="3.代码解读"></a>3.代码解读</h2><h3 id="3-1-主要组件的实现"><a href="#3-1-主要组件的实现" class="headerlink" title="3.1 主要组件的实现"></a>3.1 主要组件的实现</h3><h4 id="3-1-1-Transformer-模型-TransformerModel"><a href="#3-1-1-Transformer-模型-TransformerModel" class="headerlink" title="3.1.1 Transformer 模型 (TransformerModel)"></a>3.1.1 Transformer 模型 (TransformerModel)</h4><p>TransformerModel类是整个模型的核心，它是基于 PyTorch 提供的 nn.TransformerEncoder 实现的。<br>输入：输入的文本会被嵌入为词向量（通过 nn.Embedding），加上位置编码（PositionalEncoding）。<br>主要模块：<br>TransformerEncoder：堆叠多个 Transformer 编码器层（TransformerEncoderLayer）。<br>PositionalEncoding：为序列中每个位置添加位置信息，以保留单词之间的相对顺序。<br>Linear 层（解码器）：将编码器的输出映射到词表大小的向量，用于预测下一个词。<br>初始化权重：使用 init_weights 方法对嵌入层和解码层的权重进行初始化。</p>
<h4 id="3-1-2-位置编码-PositionalEncoding"><a href="#3-1-2-位置编码-PositionalEncoding" class="headerlink" title="3.1.2 位置编码 (PositionalEncoding)"></a>3.1.2 位置编码 (PositionalEncoding)</h4><p>位置编码为输入的嵌入向量添加位置信息。<br>使用正弦和余弦函数计算不同维度的编码，确保编码可以保留序列的相对位置关系。</p>
<h4 id="3-1-3-掩码-generate-square-subsequent-mask"><a href="#3-1-3-掩码-generate-square-subsequent-mask" class="headerlink" title="3.1.3 掩码 (generate_square_subsequent_mask)"></a>3.1.3 掩码 (generate_square_subsequent_mask)</h4><p>生成一个上三角矩阵，用于屏蔽序列中未来时间步的输入，确保 Transformer 模型仅能看到之前的时间步。</p>
<h3 id="3-2-数据加载与预处理"><a href="#3-2-数据加载与预处理" class="headerlink" title="3.2 数据加载与预处理"></a>3.2 数据加载与预处理</h3><h4 id="3-2-1-本地数据加载-load-and-preprocess-data"><a href="#3-2-1-本地数据加载-load-and-preprocess-data" class="headerlink" title="3.2.1 本地数据加载 (load_and_preprocess_data)"></a>3.2.1 本地数据加载 (load_and_preprocess_data)</h4><p>从本地目录中加载 Wikitext-2 数据集，包括 wiki.train.tokens, wiki.valid.tokens, 和 wiki.test.tokens。<br>预处理：<br>使用 basic_english 分词器对文本进行分词。<br>使用 TorchText 的 build_vocab_from_iterator 创建词表，并将所有单词映射为索引。<br>将文本转化为 PyTorch 张量。</p>
<h4 id="3-2-2-批处理函数-batchify"><a href="#3-2-2-批处理函数-batchify" class="headerlink" title="3.2.2 批处理函数 (batchify)"></a>3.2.2 批处理函数 (batchify)</h4><p>将整个数据集分割为多个固定长度的批次。<br>数据被转置为形状 [seq_len, batch_size]，便于 Transformer 模型的处理。</p>
<h3 id="3-3-训练和评估函数"><a href="#3-3-训练和评估函数" class="headerlink" title="3.3 训练和评估函数"></a>3.3 训练和评估函数</h3><h4 id="3-3-1-训练函数-train"><a href="#3-3-1-训练函数-train" class="headerlink" title="3.3.1 训练函数 (train)"></a>3.3.1 训练函数 (train)</h4><p>逐批获取输入数据，使用 generate_square_subsequent_mask 生成掩码。<br>对每个批次的输入，计算模型输出和损失：<br>损失函数：交叉熵损失 (nn.CrossEntropyLoss)。<br>使用梯度裁剪（clip_grad_norm_）防止梯度爆炸。<br>打印训练过程中的损失、困惑度（Perplexity，语言建模中的重要指标）。</p>
<h4 id="3-3-2-评估函数-evaluate"><a href="#3-3-2-评估函数-evaluate" class="headerlink" title="3.3.2 评估函数 (evaluate)"></a>3.3.2 评估函数 (evaluate)</h4><p>在验证或测试集上评估模型性能。<br>模型设置为评估模式（model.eval()），以防止 dropout 等正则化方法的干扰。</p>
<h4 id="3-3-3-数据生成-get-batch"><a href="#3-3-3-数据生成-get-batch" class="headerlink" title="3.3.3 数据生成 (get_batch)"></a>3.3.3 数据生成 (get_batch)</h4><p>生成输入和目标对：<br>输入：当前时间步的序列。<br>目标：下一时间步对应的序列。</p>
<h3 id="3-4训练过程"><a href="#3-4训练过程" class="headerlink" title="3.4训练过程"></a>3.4训练过程</h3><h4 id="3-4-1-模型超参数"><a href="#3-4-1-模型超参数" class="headerlink" title="3.4.1 模型超参数"></a>3.4.1 模型超参数</h4><p>词表大小 (ntokens)：等于数据集中所有词的数量。<br>嵌入维度 (emsize)：词向量的维度。<br>隐藏层维度 (d_hid)：Transformer 编码器的前馈网络隐藏层大小。<br>头数 (nhead)：多头注意力机制的头数。<br>层数 (nlayers)：编码器的层数。<br>dropout：用于防止过拟合的丢弃率。</p>
<h4 id="3-4-2-优化器与学习率调度器"><a href="#3-4-2-优化器与学习率调度器" class="headerlink" title="3.4.2 优化器与学习率调度器"></a>3.4.2 优化器与学习率调度器</h4><p>使用 SGD 优化器和学习率调度器 (StepLR)，每次学习率会按设定的衰减率减少。</p>
<h4 id="3-4-3-训练循环"><a href="#3-4-3-训练循环" class="headerlink" title="3.4.3 训练循环"></a>3.4.3 训练循环</h4><p>逐个 epoch 地训练模型：<br>记录训练损失和验证损失。<br>在验证集上性能更优时，保存模型的参数。</p>
<h3 id="3-5-测试模型"><a href="#3-5-测试模型" class="headerlink" title="3.5 测试模型"></a>3.5 测试模型</h3><p>加载最优模型参数并在测试集上评估。<br>打印测试集的损失和困惑度（Perplexity）。</p>
<h3 id="3-6-代码工作流程总结"><a href="#3-6-代码工作流程总结" class="headerlink" title="3.6 代码工作流程总结"></a>3.6 代码工作流程总结</h3><p>加载数据：从本地加载 Wikitext-2 数据集并进行分词和索引映射。<br>创建模型：定义并初始化 Transformer 模型。<br>训练模型：通过批量数据训练模型，记录损失和困惑度。<br>验证模型：在验证集上选择最佳模型。<br>测试模型：用测试集评估最终模型性能。</p>
<h2 id="4-运行结果"><a href="#4-运行结果" class="headerlink" title="4. 运行结果"></a>4. 运行结果</h2><h2 id="epoch-1-200-2928-batches-lr-5-00-ms-batch-131-84-loss-8-14-ppl-3423-94-epoch-1-400-2928-batches-lr-5-00-ms-batch-140-47-loss-6-88-ppl-970-93-epoch-1-600-2928-batches-lr-5-00-ms-batch-139-09-loss-6-43-ppl-623-05-epoch-1-800-2928-batches-lr-5-00-ms-batch-135-78-loss-6-30-ppl-545-26-epoch-1-1000-2928-batches-lr-5-00-ms-batch-138-33-loss-6-19-ppl-486-50-epoch-1-1200-2928-batches-lr-5-00-ms-batch-140-05-loss-6-15-ppl-467-70-epoch-1-1400-2928-batches-lr-5-00-ms-batch-142-60-loss-6-11-ppl-451-95-epoch-1-1600-2928-batches-lr-5-00-ms-batch-140-72-loss-6-10-ppl-448-07-epoch-1-1800-2928-batches-lr-5-00-ms-batch-143-08-loss-6-02-ppl-410-79-epoch-1-2000-2928-batches-lr-5-00-ms-batch-146-73-loss-6-02-ppl-410-46-epoch-1-2200-2928-batches-lr-5-00-ms-batch-149-91-loss-5-90-ppl-364-34-epoch-1-2400-2928-batches-lr-5-00-ms-batch-152-67-loss-5-97-ppl-391-99-epoch-1-2600-2928-batches-lr-5-00-ms-batch-153-82-loss-5-95-ppl-383-01-epoch-1-2800-2928-batches-lr-5-00-ms-batch-156-59-loss-5-88-ppl-358-70"><a href="#epoch-1-200-2928-batches-lr-5-00-ms-batch-131-84-loss-8-14-ppl-3423-94-epoch-1-400-2928-batches-lr-5-00-ms-batch-140-47-loss-6-88-ppl-970-93-epoch-1-600-2928-batches-lr-5-00-ms-batch-139-09-loss-6-43-ppl-623-05-epoch-1-800-2928-batches-lr-5-00-ms-batch-135-78-loss-6-30-ppl-545-26-epoch-1-1000-2928-batches-lr-5-00-ms-batch-138-33-loss-6-19-ppl-486-50-epoch-1-1200-2928-batches-lr-5-00-ms-batch-140-05-loss-6-15-ppl-467-70-epoch-1-1400-2928-batches-lr-5-00-ms-batch-142-60-loss-6-11-ppl-451-95-epoch-1-1600-2928-batches-lr-5-00-ms-batch-140-72-loss-6-10-ppl-448-07-epoch-1-1800-2928-batches-lr-5-00-ms-batch-143-08-loss-6-02-ppl-410-79-epoch-1-2000-2928-batches-lr-5-00-ms-batch-146-73-loss-6-02-ppl-410-46-epoch-1-2200-2928-batches-lr-5-00-ms-batch-149-91-loss-5-90-ppl-364-34-epoch-1-2400-2928-batches-lr-5-00-ms-batch-152-67-loss-5-97-ppl-391-99-epoch-1-2600-2928-batches-lr-5-00-ms-batch-153-82-loss-5-95-ppl-383-01-epoch-1-2800-2928-batches-lr-5-00-ms-batch-156-59-loss-5-88-ppl-358-70" class="headerlink" title="| epoch   1 |   200&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 131.84 | loss  8.14 | ppl  3423.94| epoch   1 |   400&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 140.47 | loss  6.88 | ppl   970.93| epoch   1 |   600&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 139.09 | loss  6.43 | ppl   623.05| epoch   1 |   800&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 135.78 | loss  6.30 | ppl   545.26| epoch   1 |  1000&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 138.33 | loss  6.19 | ppl   486.50| epoch   1 |  1200&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 140.05 | loss  6.15 | ppl   467.70| epoch   1 |  1400&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 142.60 | loss  6.11 | ppl   451.95| epoch   1 |  1600&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 140.72 | loss  6.10 | ppl   448.07| epoch   1 |  1800&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 143.08 | loss  6.02 | ppl   410.79| epoch   1 |  2000&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 146.73 | loss  6.02 | ppl   410.46| epoch   1 |  2200&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 149.91 | loss  5.90 | ppl   364.34| epoch   1 |  2400&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 152.67 | loss  5.97 | ppl   391.99| epoch   1 |  2600&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 153.82 | loss  5.95 | ppl   383.01| epoch   1 |  2800&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 156.59 | loss  5.88 | ppl   358.70"></a>| epoch   1 |   200&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 131.84 | loss  8.14 | ppl  3423.94<br>| epoch   1 |   400&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 140.47 | loss  6.88 | ppl   970.93<br>| epoch   1 |   600&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 139.09 | loss  6.43 | ppl   623.05<br>| epoch   1 |   800&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 135.78 | loss  6.30 | ppl   545.26<br>| epoch   1 |  1000&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 138.33 | loss  6.19 | ppl   486.50<br>| epoch   1 |  1200&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 140.05 | loss  6.15 | ppl   467.70<br>| epoch   1 |  1400&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 142.60 | loss  6.11 | ppl   451.95<br>| epoch   1 |  1600&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 140.72 | loss  6.10 | ppl   448.07<br>| epoch   1 |  1800&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 143.08 | loss  6.02 | ppl   410.79<br>| epoch   1 |  2000&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 146.73 | loss  6.02 | ppl   410.46<br>| epoch   1 |  2200&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 149.91 | loss  5.90 | ppl   364.34<br>| epoch   1 |  2400&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 152.67 | loss  5.97 | ppl   391.99<br>| epoch   1 |  2600&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 153.82 | loss  5.95 | ppl   383.01<br>| epoch   1 |  2800&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 156.59 | loss  5.88 | ppl   358.70</h2><h2 id="end-of-epoch-1-time-441-33s-valid-loss-5-81-valid-ppl-333-35"><a href="#end-of-epoch-1-time-441-33s-valid-loss-5-81-valid-ppl-333-35" class="headerlink" title="| end of epoch   1 | time: 441.33s | valid loss  5.81 | valid ppl   333.35"></a>| end of epoch   1 | time: 441.33s | valid loss  5.81 | valid ppl   333.35</h2><h2 id="epoch-2-200-2928-batches-lr-4-75-ms-batch-158-28-loss-5-86-ppl-351-74-epoch-2-400-2928-batches-lr-4-75-ms-batch-157-82-loss-5-84-ppl-345-05-epoch-2-600-2928-batches-lr-4-75-ms-batch-158-74-loss-5-67-ppl-288-98-epoch-2-800-2928-batches-lr-4-75-ms-batch-160-91-loss-5-71-ppl-301-57-epoch-2-1000-2928-batches-lr-4-75-ms-batch-160-65-loss-5-66-ppl-286-73-epoch-2-1200-2928-batches-lr-4-75-ms-batch-161-34-loss-5-68-ppl-294-02-epoch-2-1400-2928-batches-lr-4-75-ms-batch-161-62-loss-5-69-ppl-296-31-epoch-2-1600-2928-batches-lr-4-75-ms-batch-162-15-loss-5-71-ppl-302-16-epoch-2-1800-2928-batches-lr-4-75-ms-batch-161-82-loss-5-65-ppl-285-07-epoch-2-2000-2928-batches-lr-4-75-ms-batch-161-89-loss-5-68-ppl-291-94-epoch-2-2200-2928-batches-lr-4-75-ms-batch-161-86-loss-5-56-ppl-258-91-epoch-2-2400-2928-batches-lr-4-75-ms-batch-162-87-loss-5-65-ppl-282-90-epoch-2-2600-2928-batches-lr-4-75-ms-batch-161-08-loss-5-65-ppl-284-46-epoch-2-2800-2928-batches-lr-4-75-ms-batch-161-55-loss-5-58-ppl-264-79"><a href="#epoch-2-200-2928-batches-lr-4-75-ms-batch-158-28-loss-5-86-ppl-351-74-epoch-2-400-2928-batches-lr-4-75-ms-batch-157-82-loss-5-84-ppl-345-05-epoch-2-600-2928-batches-lr-4-75-ms-batch-158-74-loss-5-67-ppl-288-98-epoch-2-800-2928-batches-lr-4-75-ms-batch-160-91-loss-5-71-ppl-301-57-epoch-2-1000-2928-batches-lr-4-75-ms-batch-160-65-loss-5-66-ppl-286-73-epoch-2-1200-2928-batches-lr-4-75-ms-batch-161-34-loss-5-68-ppl-294-02-epoch-2-1400-2928-batches-lr-4-75-ms-batch-161-62-loss-5-69-ppl-296-31-epoch-2-1600-2928-batches-lr-4-75-ms-batch-162-15-loss-5-71-ppl-302-16-epoch-2-1800-2928-batches-lr-4-75-ms-batch-161-82-loss-5-65-ppl-285-07-epoch-2-2000-2928-batches-lr-4-75-ms-batch-161-89-loss-5-68-ppl-291-94-epoch-2-2200-2928-batches-lr-4-75-ms-batch-161-86-loss-5-56-ppl-258-91-epoch-2-2400-2928-batches-lr-4-75-ms-batch-162-87-loss-5-65-ppl-282-90-epoch-2-2600-2928-batches-lr-4-75-ms-batch-161-08-loss-5-65-ppl-284-46-epoch-2-2800-2928-batches-lr-4-75-ms-batch-161-55-loss-5-58-ppl-264-79" class="headerlink" title="| epoch   2 |   200&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 158.28 | loss  5.86 | ppl   351.74| epoch   2 |   400&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 157.82 | loss  5.84 | ppl   345.05| epoch   2 |   600&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 158.74 | loss  5.67 | ppl   288.98| epoch   2 |   800&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 160.91 | loss  5.71 | ppl   301.57| epoch   2 |  1000&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 160.65 | loss  5.66 | ppl   286.73| epoch   2 |  1200&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 161.34 | loss  5.68 | ppl   294.02| epoch   2 |  1400&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 161.62 | loss  5.69 | ppl   296.31| epoch   2 |  1600&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 162.15 | loss  5.71 | ppl   302.16| epoch   2 |  1800&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 161.82 | loss  5.65 | ppl   285.07| epoch   2 |  2000&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 161.89 | loss  5.68 | ppl   291.94| epoch   2 |  2200&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 161.86 | loss  5.56 | ppl   258.91| epoch   2 |  2400&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 162.87 | loss  5.65 | ppl   282.90| epoch   2 |  2600&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 161.08 | loss  5.65 | ppl   284.46| epoch   2 |  2800&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 161.55 | loss  5.58 | ppl   264.79"></a>| epoch   2 |   200&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 158.28 | loss  5.86 | ppl   351.74<br>| epoch   2 |   400&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 157.82 | loss  5.84 | ppl   345.05<br>| epoch   2 |   600&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 158.74 | loss  5.67 | ppl   288.98<br>| epoch   2 |   800&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 160.91 | loss  5.71 | ppl   301.57<br>| epoch   2 |  1000&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 160.65 | loss  5.66 | ppl   286.73<br>| epoch   2 |  1200&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 161.34 | loss  5.68 | ppl   294.02<br>| epoch   2 |  1400&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 161.62 | loss  5.69 | ppl   296.31<br>| epoch   2 |  1600&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 162.15 | loss  5.71 | ppl   302.16<br>| epoch   2 |  1800&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 161.82 | loss  5.65 | ppl   285.07<br>| epoch   2 |  2000&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 161.89 | loss  5.68 | ppl   291.94<br>| epoch   2 |  2200&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 161.86 | loss  5.56 | ppl   258.91<br>| epoch   2 |  2400&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 162.87 | loss  5.65 | ppl   282.90<br>| epoch   2 |  2600&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 161.08 | loss  5.65 | ppl   284.46<br>| epoch   2 |  2800&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 161.55 | loss  5.58 | ppl   264.79</h2><h2 id="end-of-epoch-2-time-489-75s-valid-loss-5-63-valid-ppl-279-19"><a href="#end-of-epoch-2-time-489-75s-valid-loss-5-63-valid-ppl-279-19" class="headerlink" title="| end of epoch   2 | time: 489.75s | valid loss  5.63 | valid ppl   279.19"></a>| end of epoch   2 | time: 489.75s | valid loss  5.63 | valid ppl   279.19</h2><h2 id="epoch-3-200-2928-batches-lr-4-51-ms-batch-161-83-loss-5-60-ppl-270-67-epoch-3-400-2928-batches-lr-4-51-ms-batch-161-85-loss-5-63-ppl-278-09-epoch-3-600-2928-batches-lr-4-51-ms-batch-161-18-loss-5-44-ppl-229-32-epoch-3-800-2928-batches-lr-4-51-ms-batch-160-05-loss-5-49-ppl-241-14-epoch-3-1000-2928-batches-lr-4-51-ms-batch-159-72-loss-5-44-ppl-231-02-epoch-3-1200-2928-batches-lr-4-51-ms-batch-160-88-loss-5-48-ppl-240-77-epoch-3-1400-2928-batches-lr-4-51-ms-batch-159-98-loss-5-50-ppl-243-48-epoch-3-1600-2928-batches-lr-4-51-ms-batch-159-63-loss-5-53-ppl-252-02-epoch-3-1800-2928-batches-lr-4-51-ms-batch-160-42-loss-5-47-ppl-236-82-epoch-3-2000-2928-batches-lr-4-51-ms-batch-158-95-loss-5-48-ppl-240-99-epoch-3-2200-2928-batches-lr-4-51-ms-batch-156-87-loss-5-37-ppl-214-50-epoch-3-2400-2928-batches-lr-4-51-ms-batch-157-96-loss-5-47-ppl-236-30-epoch-3-2600-2928-batches-lr-4-51-ms-batch-157-73-loss-5-47-ppl-238-29-epoch-3-2800-2928-batches-lr-4-51-ms-batch-374-93-loss-5-41-ppl-223-21"><a href="#epoch-3-200-2928-batches-lr-4-51-ms-batch-161-83-loss-5-60-ppl-270-67-epoch-3-400-2928-batches-lr-4-51-ms-batch-161-85-loss-5-63-ppl-278-09-epoch-3-600-2928-batches-lr-4-51-ms-batch-161-18-loss-5-44-ppl-229-32-epoch-3-800-2928-batches-lr-4-51-ms-batch-160-05-loss-5-49-ppl-241-14-epoch-3-1000-2928-batches-lr-4-51-ms-batch-159-72-loss-5-44-ppl-231-02-epoch-3-1200-2928-batches-lr-4-51-ms-batch-160-88-loss-5-48-ppl-240-77-epoch-3-1400-2928-batches-lr-4-51-ms-batch-159-98-loss-5-50-ppl-243-48-epoch-3-1600-2928-batches-lr-4-51-ms-batch-159-63-loss-5-53-ppl-252-02-epoch-3-1800-2928-batches-lr-4-51-ms-batch-160-42-loss-5-47-ppl-236-82-epoch-3-2000-2928-batches-lr-4-51-ms-batch-158-95-loss-5-48-ppl-240-99-epoch-3-2200-2928-batches-lr-4-51-ms-batch-156-87-loss-5-37-ppl-214-50-epoch-3-2400-2928-batches-lr-4-51-ms-batch-157-96-loss-5-47-ppl-236-30-epoch-3-2600-2928-batches-lr-4-51-ms-batch-157-73-loss-5-47-ppl-238-29-epoch-3-2800-2928-batches-lr-4-51-ms-batch-374-93-loss-5-41-ppl-223-21" class="headerlink" title="| epoch   3 |   200&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 161.83 | loss  5.60 | ppl   270.67| epoch   3 |   400&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 161.85 | loss  5.63 | ppl   278.09| epoch   3 |   600&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 161.18 | loss  5.44 | ppl   229.32| epoch   3 |   800&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 160.05 | loss  5.49 | ppl   241.14| epoch   3 |  1000&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 159.72 | loss  5.44 | ppl   231.02| epoch   3 |  1200&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 160.88 | loss  5.48 | ppl   240.77| epoch   3 |  1400&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 159.98 | loss  5.50 | ppl   243.48| epoch   3 |  1600&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 159.63 | loss  5.53 | ppl   252.02| epoch   3 |  1800&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 160.42 | loss  5.47 | ppl   236.82| epoch   3 |  2000&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 158.95 | loss  5.48 | ppl   240.99| epoch   3 |  2200&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 156.87 | loss  5.37 | ppl   214.50| epoch   3 |  2400&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 157.96 | loss  5.47 | ppl   236.30| epoch   3 |  2600&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 157.73 | loss  5.47 | ppl   238.29| epoch   3 |  2800&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 374.93 | loss  5.41 | ppl   223.21"></a>| epoch   3 |   200&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 161.83 | loss  5.60 | ppl   270.67<br>| epoch   3 |   400&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 161.85 | loss  5.63 | ppl   278.09<br>| epoch   3 |   600&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 161.18 | loss  5.44 | ppl   229.32<br>| epoch   3 |   800&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 160.05 | loss  5.49 | ppl   241.14<br>| epoch   3 |  1000&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 159.72 | loss  5.44 | ppl   231.02<br>| epoch   3 |  1200&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 160.88 | loss  5.48 | ppl   240.77<br>| epoch   3 |  1400&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 159.98 | loss  5.50 | ppl   243.48<br>| epoch   3 |  1600&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 159.63 | loss  5.53 | ppl   252.02<br>| epoch   3 |  1800&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 160.42 | loss  5.47 | ppl   236.82<br>| epoch   3 |  2000&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 158.95 | loss  5.48 | ppl   240.99<br>| epoch   3 |  2200&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 156.87 | loss  5.37 | ppl   214.50<br>| epoch   3 |  2400&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 157.96 | loss  5.47 | ppl   236.30<br>| epoch   3 |  2600&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 157.73 | loss  5.47 | ppl   238.29<br>| epoch   3 |  2800&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 374.93 | loss  5.41 | ppl   223.21</h2><h2 id="end-of-epoch-3-time-623-88s-valid-loss-5-60-valid-ppl-270-49"><a href="#end-of-epoch-3-time-623-88s-valid-loss-5-60-valid-ppl-270-49" class="headerlink" title="| end of epoch   3 | time: 623.88s | valid loss  5.60 | valid ppl   270.49"></a>| end of epoch   3 | time: 623.88s | valid loss  5.60 | valid ppl   270.49</h2><h1 id="End-of-training-test-loss-5-51-test-ppl-246-59"><a href="#End-of-training-test-loss-5-51-test-ppl-246-59" class="headerlink" title="&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;| End of training | test loss  5.51 | test ppl   246.59"></a>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;<br>| End of training | test loss  5.51 | test ppl   246.59</h1></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://SEER666.github.io">SEER</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://seer666.github.io/2024/11/24/2024112402/">https://seer666.github.io/2024/11/24/2024112402/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://SEER666.github.io" target="_blank">SEER's Study</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E4%BB%A3%E7%A0%81/">代码</a></div><div class="post-share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/11/24/2024112401/" title="Transformer文档阅读"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Transformer文档阅读</div></div><div class="info-2"><div class="info-item-1">   window.MathJax = {     tex: {       inlineMath: [['$', '$'], ['\\(', '\\)']],         displayMath: [['$$', '$$'], ['\\[', '\\]']]       }   };    1.torch.nn.Transformerclasstorch.nn.Transformer(d_model&#x3D;512, nhead&#x3D;8, num_encoder_layers&#x3D;6, num_decoder_layers&#x3D;6, dim_feedforward&#x3D;2048, dropout&#x3D;0.1, activation&#x3D;, custom_encoder&#x3D;None, custom_decoder&#x3D;None, layer_norm_eps&#x3D;1e-05, batch_first&#x3D;False, norm_first&#x3D;False, bias&#x3D;True,...</div></div></div></a><a class="pagination-related" href="/2024/11/24/20241124/" title="改进具有动态可组合多头注意力的Transformer"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">改进具有动态可组合多头注意力的Transformer</div></div><div class="info-2"><div class="info-item-1">   window.MathJax = {     tex: {       inlineMath: [['$', '$'], ['\\(', '\\)']],         displayMath: [['$$', '$$'], ['\\[', '\\]']]       }  ...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/11/24/2024112401/" title="Transformer文档阅读"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="info-item-2">Transformer文档阅读</div></div><div class="info-2"><div class="info-item-1">   window.MathJax = {     tex: {       inlineMath: [['$', '$'], ['\\(', '\\)']],         displayMath: [['$$', '$$'], ['\\[', '\\]']]       }   };    1.torch.nn.Transformerclasstorch.nn.Transformer(d_model&#x3D;512, nhead&#x3D;8, num_encoder_layers&#x3D;6, num_decoder_layers&#x3D;6, dim_feedforward&#x3D;2048, dropout&#x3D;0.1, activation&#x3D;, custom_encoder&#x3D;None, custom_decoder&#x3D;None, layer_norm_eps&#x3D;1e-05, batch_first&#x3D;False, norm_first&#x3D;False, bias&#x3D;True,...</div></div></div></a><a class="pagination-related" href="/2024/11/28/20241128/" title="DC Former代码框架思路解析"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-28</div><div class="info-item-2">DC Former代码框架思路解析</div></div><div class="info-2"><div class="info-item-1">1 DCFormer1.1 config.json本文件是DCFormer 模型的配置文件，定义了模型的各种超参数和设定。 -字段说明**architectures:**模型架构的名称，这里是 “DCFormer”。**auto_map:**自动映射模型和配置类，指定了 AutoConfig 和 AutoModelForCausalLM 对应的配置类和模型类路径。AutoConfig: 指定 DCFormerConfig，这是模型的配置类。**AutoModelForCausalLM:**指定 DCFormer 模型类，这是实际的推理模型。**block_size:**输入序列的最大长度（2048），影响模型在训练或推理时处理的上下文窗口大小。**bos_token_id 和 eos_token_id:**这是模型的特殊标记（begin-of-sequence 和 end-of-sequence），分别对应词汇表中的 1 和 2。**dim:**模型的隐藏层维度，设置为 2560。**head_dim:**每个注意力头的维度，设置为...</div></div></div></a><a class="pagination-related" href="/2025/01/09/20250109/" title="CSAPP考前题"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-09</div><div class="info-item-2">CSAPP考前题</div></div><div class="info-2"><div class="info-item-1">   window.MathJax = {     tex: {       inlineMath: [['$', '$'], ['\\(', '\\)']],         displayMath: [['$$', '$$'], ['\\[', '\\]']]       }   };    一、系统漫游线上测试1.以下说法正确的是（   ）。a.处理器顺序执行机器指令。b.主存储器包括寄存器。c.总线系统只用来传输数据，不传输指令。d.中央处理器（CPU）是特定指令集架构下的执行单元。 答案：D解析：  a项，目前多半是指令级并行、流水线技术，未必顺序执行； b项，著储存器与寄存器是两种不同的存储类型。寄存器是CPU内部的高速存储单元，主要用于临时存储数据和指令，而主存储器是外部于CPU的，通常用来存储程序和数据。 c项，总线系统可以传输指令。  2.可执行目标程序是（   ）。a.在目标机运行的汇编语言程序。b.是机器指令被按照固定格式打包的二进制文件。c.由编译器产生的汇编程序。d.在目标机运行的高级语言程序。 答案：B 3.SHELL的功能是（  ...</div></div></div></a></div></div><!-- 添加 Valine 评论系统的 HTML 容器--><!-- Valine 评论系统--><div id="vcomments"></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="https://avatars.githubusercontent.com/u/126209991?v=4" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">SEER</div><div class="author-info-description">Record SEER's learning content.</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">40</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/SEER666"><i class="fab fa-github"></i><span>关注我</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">我的博客已全面升级</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BB%A3%E7%A0%81%E5%B1%95%E7%A4%BA"><span class="toc-number">1.</span> <span class="toc-text">1.代码展示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD"><span class="toc-number">2.</span> <span class="toc-text">2.数据集下载</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB"><span class="toc-number">3.</span> <span class="toc-text">3.代码解读</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E4%B8%BB%E8%A6%81%E7%BB%84%E4%BB%B6%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 主要组件的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-1-Transformer-%E6%A8%A1%E5%9E%8B-TransformerModel"><span class="toc-number">3.1.1.</span> <span class="toc-text">3.1.1 Transformer 模型 (TransformerModel)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-2-%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81-PositionalEncoding"><span class="toc-number">3.1.2.</span> <span class="toc-text">3.1.2 位置编码 (PositionalEncoding)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-3-%E6%8E%A9%E7%A0%81-generate-square-subsequent-mask"><span class="toc-number">3.1.3.</span> <span class="toc-text">3.1.3 掩码 (generate_square_subsequent_mask)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E4%B8%8E%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 数据加载与预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-%E6%9C%AC%E5%9C%B0%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD-load-and-preprocess-data"><span class="toc-number">3.2.1.</span> <span class="toc-text">3.2.1 本地数据加载 (load_and_preprocess_data)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2-%E6%89%B9%E5%A4%84%E7%90%86%E5%87%BD%E6%95%B0-batchify"><span class="toc-number">3.2.2.</span> <span class="toc-text">3.2.2 批处理函数 (batchify)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E8%AE%AD%E7%BB%83%E5%92%8C%E8%AF%84%E4%BC%B0%E5%87%BD%E6%95%B0"><span class="toc-number">3.3.</span> <span class="toc-text">3.3 训练和评估函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-1-%E8%AE%AD%E7%BB%83%E5%87%BD%E6%95%B0-train"><span class="toc-number">3.3.1.</span> <span class="toc-text">3.3.1 训练函数 (train)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-2-%E8%AF%84%E4%BC%B0%E5%87%BD%E6%95%B0-evaluate"><span class="toc-number">3.3.2.</span> <span class="toc-text">3.3.2 评估函数 (evaluate)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-3-%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90-get-batch"><span class="toc-number">3.3.3.</span> <span class="toc-text">3.3.3 数据生成 (get_batch)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-number">3.4.</span> <span class="toc-text">3.4训练过程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-1-%E6%A8%A1%E5%9E%8B%E8%B6%85%E5%8F%82%E6%95%B0"><span class="toc-number">3.4.1.</span> <span class="toc-text">3.4.1 模型超参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-2-%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%8E%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="toc-number">3.4.2.</span> <span class="toc-text">3.4.2 优化器与学习率调度器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-3-%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF"><span class="toc-number">3.4.3.</span> <span class="toc-text">3.4.3 训练循环</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-%E6%B5%8B%E8%AF%95%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.5.</span> <span class="toc-text">3.5 测试模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-%E4%BB%A3%E7%A0%81%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E6%80%BB%E7%BB%93"><span class="toc-number">3.6.</span> <span class="toc-text">3.6 代码工作流程总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E8%BF%90%E8%A1%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">4.</span> <span class="toc-text">4. 运行结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#epoch-1-200-2928-batches-lr-5-00-ms-batch-131-84-loss-8-14-ppl-3423-94-epoch-1-400-2928-batches-lr-5-00-ms-batch-140-47-loss-6-88-ppl-970-93-epoch-1-600-2928-batches-lr-5-00-ms-batch-139-09-loss-6-43-ppl-623-05-epoch-1-800-2928-batches-lr-5-00-ms-batch-135-78-loss-6-30-ppl-545-26-epoch-1-1000-2928-batches-lr-5-00-ms-batch-138-33-loss-6-19-ppl-486-50-epoch-1-1200-2928-batches-lr-5-00-ms-batch-140-05-loss-6-15-ppl-467-70-epoch-1-1400-2928-batches-lr-5-00-ms-batch-142-60-loss-6-11-ppl-451-95-epoch-1-1600-2928-batches-lr-5-00-ms-batch-140-72-loss-6-10-ppl-448-07-epoch-1-1800-2928-batches-lr-5-00-ms-batch-143-08-loss-6-02-ppl-410-79-epoch-1-2000-2928-batches-lr-5-00-ms-batch-146-73-loss-6-02-ppl-410-46-epoch-1-2200-2928-batches-lr-5-00-ms-batch-149-91-loss-5-90-ppl-364-34-epoch-1-2400-2928-batches-lr-5-00-ms-batch-152-67-loss-5-97-ppl-391-99-epoch-1-2600-2928-batches-lr-5-00-ms-batch-153-82-loss-5-95-ppl-383-01-epoch-1-2800-2928-batches-lr-5-00-ms-batch-156-59-loss-5-88-ppl-358-70"><span class="toc-number">5.</span> <span class="toc-text">| epoch   1 |   200&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 131.84 | loss  8.14 | ppl  3423.94| epoch   1 |   400&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 140.47 | loss  6.88 | ppl   970.93| epoch   1 |   600&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 139.09 | loss  6.43 | ppl   623.05| epoch   1 |   800&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 135.78 | loss  6.30 | ppl   545.26| epoch   1 |  1000&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 138.33 | loss  6.19 | ppl   486.50| epoch   1 |  1200&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 140.05 | loss  6.15 | ppl   467.70| epoch   1 |  1400&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 142.60 | loss  6.11 | ppl   451.95| epoch   1 |  1600&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 140.72 | loss  6.10 | ppl   448.07| epoch   1 |  1800&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 143.08 | loss  6.02 | ppl   410.79| epoch   1 |  2000&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 146.73 | loss  6.02 | ppl   410.46| epoch   1 |  2200&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 149.91 | loss  5.90 | ppl   364.34| epoch   1 |  2400&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 152.67 | loss  5.97 | ppl   391.99| epoch   1 |  2600&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 153.82 | loss  5.95 | ppl   383.01| epoch   1 |  2800&#x2F; 2928 batches | lr 5.00 | ms&#x2F;batch 156.59 | loss  5.88 | ppl   358.70</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#end-of-epoch-1-time-441-33s-valid-loss-5-81-valid-ppl-333-35"><span class="toc-number">6.</span> <span class="toc-text">| end of epoch   1 | time: 441.33s | valid loss  5.81 | valid ppl   333.35</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#epoch-2-200-2928-batches-lr-4-75-ms-batch-158-28-loss-5-86-ppl-351-74-epoch-2-400-2928-batches-lr-4-75-ms-batch-157-82-loss-5-84-ppl-345-05-epoch-2-600-2928-batches-lr-4-75-ms-batch-158-74-loss-5-67-ppl-288-98-epoch-2-800-2928-batches-lr-4-75-ms-batch-160-91-loss-5-71-ppl-301-57-epoch-2-1000-2928-batches-lr-4-75-ms-batch-160-65-loss-5-66-ppl-286-73-epoch-2-1200-2928-batches-lr-4-75-ms-batch-161-34-loss-5-68-ppl-294-02-epoch-2-1400-2928-batches-lr-4-75-ms-batch-161-62-loss-5-69-ppl-296-31-epoch-2-1600-2928-batches-lr-4-75-ms-batch-162-15-loss-5-71-ppl-302-16-epoch-2-1800-2928-batches-lr-4-75-ms-batch-161-82-loss-5-65-ppl-285-07-epoch-2-2000-2928-batches-lr-4-75-ms-batch-161-89-loss-5-68-ppl-291-94-epoch-2-2200-2928-batches-lr-4-75-ms-batch-161-86-loss-5-56-ppl-258-91-epoch-2-2400-2928-batches-lr-4-75-ms-batch-162-87-loss-5-65-ppl-282-90-epoch-2-2600-2928-batches-lr-4-75-ms-batch-161-08-loss-5-65-ppl-284-46-epoch-2-2800-2928-batches-lr-4-75-ms-batch-161-55-loss-5-58-ppl-264-79"><span class="toc-number">7.</span> <span class="toc-text">| epoch   2 |   200&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 158.28 | loss  5.86 | ppl   351.74| epoch   2 |   400&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 157.82 | loss  5.84 | ppl   345.05| epoch   2 |   600&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 158.74 | loss  5.67 | ppl   288.98| epoch   2 |   800&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 160.91 | loss  5.71 | ppl   301.57| epoch   2 |  1000&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 160.65 | loss  5.66 | ppl   286.73| epoch   2 |  1200&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 161.34 | loss  5.68 | ppl   294.02| epoch   2 |  1400&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 161.62 | loss  5.69 | ppl   296.31| epoch   2 |  1600&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 162.15 | loss  5.71 | ppl   302.16| epoch   2 |  1800&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 161.82 | loss  5.65 | ppl   285.07| epoch   2 |  2000&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 161.89 | loss  5.68 | ppl   291.94| epoch   2 |  2200&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 161.86 | loss  5.56 | ppl   258.91| epoch   2 |  2400&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 162.87 | loss  5.65 | ppl   282.90| epoch   2 |  2600&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 161.08 | loss  5.65 | ppl   284.46| epoch   2 |  2800&#x2F; 2928 batches | lr 4.75 | ms&#x2F;batch 161.55 | loss  5.58 | ppl   264.79</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#end-of-epoch-2-time-489-75s-valid-loss-5-63-valid-ppl-279-19"><span class="toc-number">8.</span> <span class="toc-text">| end of epoch   2 | time: 489.75s | valid loss  5.63 | valid ppl   279.19</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#epoch-3-200-2928-batches-lr-4-51-ms-batch-161-83-loss-5-60-ppl-270-67-epoch-3-400-2928-batches-lr-4-51-ms-batch-161-85-loss-5-63-ppl-278-09-epoch-3-600-2928-batches-lr-4-51-ms-batch-161-18-loss-5-44-ppl-229-32-epoch-3-800-2928-batches-lr-4-51-ms-batch-160-05-loss-5-49-ppl-241-14-epoch-3-1000-2928-batches-lr-4-51-ms-batch-159-72-loss-5-44-ppl-231-02-epoch-3-1200-2928-batches-lr-4-51-ms-batch-160-88-loss-5-48-ppl-240-77-epoch-3-1400-2928-batches-lr-4-51-ms-batch-159-98-loss-5-50-ppl-243-48-epoch-3-1600-2928-batches-lr-4-51-ms-batch-159-63-loss-5-53-ppl-252-02-epoch-3-1800-2928-batches-lr-4-51-ms-batch-160-42-loss-5-47-ppl-236-82-epoch-3-2000-2928-batches-lr-4-51-ms-batch-158-95-loss-5-48-ppl-240-99-epoch-3-2200-2928-batches-lr-4-51-ms-batch-156-87-loss-5-37-ppl-214-50-epoch-3-2400-2928-batches-lr-4-51-ms-batch-157-96-loss-5-47-ppl-236-30-epoch-3-2600-2928-batches-lr-4-51-ms-batch-157-73-loss-5-47-ppl-238-29-epoch-3-2800-2928-batches-lr-4-51-ms-batch-374-93-loss-5-41-ppl-223-21"><span class="toc-number">9.</span> <span class="toc-text">| epoch   3 |   200&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 161.83 | loss  5.60 | ppl   270.67| epoch   3 |   400&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 161.85 | loss  5.63 | ppl   278.09| epoch   3 |   600&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 161.18 | loss  5.44 | ppl   229.32| epoch   3 |   800&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 160.05 | loss  5.49 | ppl   241.14| epoch   3 |  1000&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 159.72 | loss  5.44 | ppl   231.02| epoch   3 |  1200&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 160.88 | loss  5.48 | ppl   240.77| epoch   3 |  1400&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 159.98 | loss  5.50 | ppl   243.48| epoch   3 |  1600&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 159.63 | loss  5.53 | ppl   252.02| epoch   3 |  1800&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 160.42 | loss  5.47 | ppl   236.82| epoch   3 |  2000&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 158.95 | loss  5.48 | ppl   240.99| epoch   3 |  2200&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 156.87 | loss  5.37 | ppl   214.50| epoch   3 |  2400&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 157.96 | loss  5.47 | ppl   236.30| epoch   3 |  2600&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 157.73 | loss  5.47 | ppl   238.29| epoch   3 |  2800&#x2F; 2928 batches | lr 4.51 | ms&#x2F;batch 374.93 | loss  5.41 | ppl   223.21</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#end-of-epoch-3-time-623-88s-valid-loss-5-60-valid-ppl-270-49"><span class="toc-number">10.</span> <span class="toc-text">| end of epoch   3 | time: 623.88s | valid loss  5.60 | valid ppl   270.49</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#End-of-training-test-loss-5-51-test-ppl-246-59"><span class="toc-number"></span> <span class="toc-text">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;| End of training | test loss  5.51 | test ppl   246.59</span></a></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/02/01/20250201_1/" title="无标题"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="无标题"/></a><div class="content"><a class="title" href="/2025/02/01/20250201_1/" title="无标题">无标题</a><time datetime="2025-02-01T07:00:00.000Z" title="发表于 2025-02-01 15:00:00">2025-02-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/01/20250201/" title="快速收敛的联邦学习与自适应权重调整"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="快速收敛的联邦学习与自适应权重调整"/></a><div class="content"><a class="title" href="/2025/02/01/20250201/" title="快速收敛的联邦学习与自适应权重调整">快速收敛的联邦学习与自适应权重调整</a><time datetime="2025-02-01T07:00:00.000Z" title="发表于 2025-02-01 15:00:00">2025-02-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/01/20250201_2/" title="学习私有神经语言模型与专注聚合"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="学习私有神经语言模型与专注聚合"/></a><div class="content"><a class="title" href="/2025/02/01/20250201_2/" title="学习私有神经语言模型与专注聚合">学习私有神经语言模型与专注聚合</a><time datetime="2025-02-01T07:00:00.000Z" title="发表于 2025-02-01 15:00:00">2025-02-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/09/20250109/" title="CSAPP考前题"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CSAPP考前题"/></a><div class="content"><a class="title" href="/2025/01/09/20250109/" title="CSAPP考前题">CSAPP考前题</a><time datetime="2025-01-09T06:30:00.000Z" title="发表于 2025-01-09 14:30:00">2025-01-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/04/20241105%20-%20%E5%89%AF%E6%9C%AC/" title="2025-CSAPP"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2025-CSAPP"/></a><div class="content"><a class="title" href="/2025/01/04/20241105%20-%20%E5%89%AF%E6%9C%AC/" title="2025-CSAPP">2025-CSAPP</a><time datetime="2025-01-04T11:00:00.000Z" title="发表于 2025-01-04 19:00:00">2025-01-04</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By SEER</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.1.0"></script><script src="/js/main.js?v=5.1.0"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (false) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@11.3.0/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/dist/click-show-text.min.js" data-mobile="true" data-text="我,太,想,进,步,了" data-fontsize="15px" data-random="false" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script><script>new Valine({
  el: '#vcomments',  // 挂载评论的 HTML 元素
  appId: 'X7VMJZiKLlND8y3EXDtaqPI5-gzGzoHsz',  // LeanCloud 的 App ID
  appKey: 'zFn5LKNiWC2vPb3y0RsW1nMK',  // LeanCloud 的 App Key
  path: window.location.pathname,  // 使用文章的路径作为评论的唯一标识
  placeholder: '留下你的评论吧...'  // 输入框的占位文本
})</script></body></html>