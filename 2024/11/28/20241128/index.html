<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>DC Former代码框架思路解析 | SEER's Study</title><meta name="author" content="SEER"><meta name="copyright" content="SEER"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="1 DCFormer1.1 config.json本文件是DCFormer 模型的配置文件，定义了模型的各种超参数和设定。 -字段说明**architectures:**模型架构的名称，这里是 “DCFormer”。**auto_map:**自动映射模型和配置类，指定了 AutoConfig 和 AutoModelForCausalLM 对应的配置类和模型类路径。AutoConfig: 指定 DC">
<meta property="og:type" content="article">
<meta property="og:title" content="DC Former代码框架思路解析">
<meta property="og:url" content="https://seer666.github.io/2024/11/28/20241128/index.html">
<meta property="og:site_name" content="SEER&#39;s Study">
<meta property="og:description" content="1 DCFormer1.1 config.json本文件是DCFormer 模型的配置文件，定义了模型的各种超参数和设定。 -字段说明**architectures:**模型架构的名称，这里是 “DCFormer”。**auto_map:**自动映射模型和配置类，指定了 AutoConfig 和 AutoModelForCausalLM 对应的配置类和模型类路径。AutoConfig: 指定 DC">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg">
<meta property="article:published_time" content="2024-11-28T13:30:00.000Z">
<meta property="article:modified_time" content="2024-11-30T08:00:31.664Z">
<meta property="article:author" content="SEER">
<meta property="article:tag" content="代码">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><link rel="shortcut icon" href="https://avatars.githubusercontent.com/u/126209991?v=4"><link rel="canonical" href="https://seer666.github.io/2024/11/28/20241128/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.1.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.6.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        if (name && globalFn[key][name]) return
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'DC Former代码框架思路解析',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-11-30 16:00:31'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><a href="https://github.com/SEER666" target="_blank"> <img src="https://avatars.githubusercontent.com/u/126209991?v=4" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></a></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">37</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 列表</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我们</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="https://avatars.githubusercontent.com/u/126209991?v=4" alt="Logo"><span class="site-name">SEER's Study</span></a><a class="nav-page-title" href="/"><span class="site-name">DC Former代码框架思路解析</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 列表</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我们</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">DC Former代码框架思路解析</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-11-28T13:30:00.000Z" title="发表于 2024-11-28 21:30:00">2024-11-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-11-30T08:00:31.664Z" title="更新于 2024-11-30 16:00:31">2024-11-30</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="1-DCFormer"><a href="#1-DCFormer" class="headerlink" title="1 DCFormer"></a>1 DCFormer</h2><h3 id="1-1-config-json"><a href="#1-1-config-json" class="headerlink" title="1.1 config.json"></a>1.1 config.json</h3><p>本文件是DCFormer 模型的配置文件，定义了模型的各种超参数和设定。</p>
<p>-<strong>字段说明</strong><br>**architectures:**模型架构的名称，这里是 “DCFormer”。<br>**auto_map:**自动映射模型和配置类，指定了 AutoConfig 和 AutoModelForCausalLM 对应的配置类和模型类路径。<br><strong>AutoConfig:</strong> 指定 DCFormerConfig，这是模型的配置类。<br>**AutoModelForCausalLM:**指定 DCFormer 模型类，这是实际的推理模型。<br>**block_size:**输入序列的最大长度（2048），影响模型在训练或推理时处理的上下文窗口大小。<br>**bos_token_id 和 eos_token_id:**这是模型的特殊标记（begin-of-sequence 和 end-of-sequence），分别对应词汇表中的 1 和 2。<br>**dim:**模型的隐藏层维度，设置为 2560。<br>**head_dim:**每个注意力头的维度，设置为 80。<br>**intermediate_size:**前馈网络的中间层维度，设置为 6912。<br>**n_head:**注意力头的数量，这里设置为 32。<br>**n_layer:**Transformer 层数，设置为 32。<br>**norm_eps:**归一化中的 epsilon 值，设置为 1e-6。<br>**q_chunk_size:**查询（query）的分块大小，这个参数用于控制大规模模型训练中的内存使用。<br>**use_dcmha:**是否启用动态可组合多头注意力（DCMHA），设为 true，意味着使用了 DCFormer 改进的注意力机制。<br>**use_qk_norm:**是否启用查询和键的归一化（QK Norm），设为 true，这有助于避免在训练过程中出现不稳定。<br>**vocab_size:**词汇表大小，这里设置为 50257，符合大规模模型（如 GPT 系列）的标准大小。</p>
<h3 id="1-2-configuration-dcformer-py"><a href="#1-2-configuration-dcformer-py" class="headerlink" title="1.2 configuration_dcformer.py"></a>1.2 configuration_dcformer.py</h3><p>DCFormerConfig 类继承自 PretrainedConfig，用于初始化和管理 DCFormer模型.</p>
<p>-DCFormerConfig 类初始化了模型的核心超参数，很多参数与 config.json 中的字段相对应。<br>-构造函数中的一些值会根据给定的参数进行自动推断，比如 intermediate_size（默认情况下为 None，会根据模型维度自动计算）。<br>-该类继承自 PretrainedConfig，使得模型可以无缝与 Huggingface Transformers 框架集成。</p>
<h3 id="1-3-generation-demo-py"><a href="#1-3-generation-demo-py" class="headerlink" title="1.3 generation_demo.py"></a>1.3 generation_demo.py</h3><p>本文件展示了如何使用 DCFormer 模型进行推理生成。</p>
<p>-加载预训练的 DCFormer-2.8B 模型和分词器。<br>-使用 CUDA 加速模型推理，确保计算效率。<br>-模型生成 100 个 token，并使用分词器解码生成的 ID 到文本格式。<br>-示例代码简单直观，演示了如何利用 Huggingface AutoModelForCausalLM 和 AutoTokenizer 接口快速加载并推理生成文本。</p>
<h3 id="1-4-maxtext2torch-py"><a href="#1-4-maxtext2torch-py" class="headerlink" title="1.4 maxtext2torch.py"></a>1.4 maxtext2torch.py</h3><p>maxtext2torch.py 文件的作用是将 MaxText 格式的模型权重转换为 PyTorch 支持的格式，通常用于加载预训练模型或进行模型迁移。MaxText 是一种特殊的模型保存格式，可能用于一些特定框架或定制模型存储。<br>将其转换为 PyTorch 格式后，可以直接使用 transformers 库加载并进行推理或微调。</p>
<p>-<strong>加载 MaxText 权重：</strong>假设 MaxText 权重是以某种特定的格式存储的，首先需要通过某种方式将其解析出来。通常这种格式会涉及到二进制文件或者自定义格式，需要根据模型开发者提供的解析规则来加载。<br>-<strong>加载 PyTorch 模型：</strong>在转换过程中，我们使用 AutoModelForCausalLM 来加载一个预训练的 PyTorch 模型，这里假设已经有一个对应的 DCFormer 模型。<br>-<strong>权重加载：</strong>将从 MaxText 格式读取的权重加载到 PyTorch 模型中，并且调用 model.load_state_dict() 方法。<br>-<strong>保存转换后的模型：</strong>将最终加载了 MaxText 权重的模型保存在 .pt 格式的文件中，以便后续使用。</p>
<h3 id="1-5-modeling-dcformer-py"><a href="#1-5-modeling-dcformer-py" class="headerlink" title="1.5 modeling_dcformer.py"></a>1.5 modeling_dcformer.py</h3><p>代码实现了 DCFormer 模型，基于 Transformer 的自注意力机制，进行了动态加权、窗口优化和分组注意力等改进，目的是优化训练和推理性能。</p>
<p>以下是核心代码解析：</p>
<h4 id="1-5-1-KVKWCache"><a href="#1-5-1-KVKWCache" class="headerlink" title="1.5.1 KVKWCache"></a>1.5.1 KVKWCache</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">KVKWCache</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, max_batch_size, max_seq_length, n_heads, head_dim, window_size=<span class="number">2048</span>, dtype=torch.float16, use_kw_cache=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.head_dim = head_dim</span><br><span class="line">        <span class="variable language_">self</span>.kw_dim = <span class="number">2</span> * n_heads </span><br><span class="line">        <span class="variable language_">self</span>.n_heads = n_heads</span><br><span class="line">        <span class="variable language_">self</span>.window_size = window_size</span><br><span class="line">        <span class="variable language_">self</span>.use_kw_cache = use_kw_cache </span><br><span class="line">        <span class="keyword">if</span> window_size <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="variable language_">self</span>.seq_length = max_seq_length</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.seq_length = <span class="built_in">min</span>(window_size, max_seq_length)</span><br><span class="line">        cache_shape = (max_batch_size, n_heads, <span class="variable language_">self</span>.seq_length, head_dim)</span><br><span class="line">        kw_cache_shape = (max_batch_size, <span class="variable language_">self</span>.seq_length, <span class="number">2</span>, n_heads, n_heads)</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;k_cache&#x27;</span>, torch.zeros(cache_shape, dtype=dtype))</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;v_cache&#x27;</span>, torch.zeros(cache_shape, dtype=dtype))</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_kw_cache:</span><br><span class="line">            <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;kw_cache&#x27;</span>, torch.zeros(kw_cache_shape, dtype=dtype))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, input_pos, k_val, v_val, kw_val=<span class="literal">None</span></span>): <span class="comment"># kw_val B,N,S,2,N      B2NSD</span></span><br><span class="line">        <span class="comment"># input_pos: [S], k_val: [B, H, S, D]</span></span><br><span class="line">        <span class="keyword">assert</span> input_pos.shape[-<span class="number">1</span>] == k_val.shape[<span class="number">2</span>]</span><br><span class="line">        B,N,S,D = v_val.shape</span><br><span class="line">        k_out = <span class="variable language_">self</span>.k_cache</span><br><span class="line">        v_out = <span class="variable language_">self</span>.v_cache</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_kw_cache:</span><br><span class="line">            kw_out = <span class="variable language_">self</span>.kw_cache</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            kw_out = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.window_size <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            k_out[:, :, input_pos] = k_val</span><br><span class="line">            v_out[:, :, input_pos] = v_val</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.use_kw_cache <span class="keyword">and</span> kw_val <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                kw_out[:,input_pos] = kw_val</span><br><span class="line">        <span class="keyword">elif</span> S == <span class="number">1</span>: </span><br><span class="line">            input_pos = input_pos % <span class="variable language_">self</span>.seq_length</span><br><span class="line">            v_out[:, :, input_pos] = v_val</span><br><span class="line">            k_out[:, :, input_pos] = k_val</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.use_kw_cache <span class="keyword">and</span> kw_val <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                kw_out[:,input_pos] = kw_val</span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># prefill</span></span><br><span class="line">            start = <span class="built_in">max</span>(<span class="number">0</span>, input_pos[-<span class="number">1</span>]-<span class="variable language_">self</span>.seq_length+<span class="number">1</span>)</span><br><span class="line">            input_pos = input_pos[start:] % <span class="variable language_">self</span>.seq_length</span><br><span class="line">            v_out[:, :, input_pos] = v_val[:,:,start:]</span><br><span class="line">            k_out[:, :, input_pos] = k_val[:,:,start:]</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.use_kw_cache <span class="keyword">and</span> kw_val <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                kw_out[:, input_pos] = kw_val[:,start:]</span><br><span class="line">        <span class="keyword">return</span> k_out, v_out, kw_out </span><br></pre></td></tr></table></figure>

<p>这部分是缓存模块，用于高效存储和更新注意力层的 Key-Value（KV）对。</p>
<p><strong>作用:</strong><br>-支持推理时的序列缓存和动态窗口机制。<br>提供可选的权重缓存 (kw_cache)，允许动态加权的注意力机制。<br><strong>主要方法:</strong><br>-update: 更新缓存，支持单 token（在线生成）或批量（填充模式）更新。<br><strong>关键逻辑:</strong><br>窗口限制 (window_size) 允许模型仅关注局部上下文。<br>支持动态生成的权重（kw_cache）。</p>
<h4 id="1-5-2-DCFormer"><a href="#1-5-2-DCFormer" class="headerlink" title="1.5.2 DCFormer"></a>1.5.2 DCFormer</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DCFormer</span>(<span class="title class_ inherited__">PreTrainedModel</span>):</span><br><span class="line">    config_class=DCFormerConfig</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    DCFormer&#x27;s implementation is adapted from https://github.com/pytorch-labs/gpt-fast/blob/main/model.py#L89 </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config: DCFormerConfig</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__(config)</span><br><span class="line">        <span class="variable language_">self</span>.config = config</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.tok_embeddings = nn.Embedding(config.vocab_size, config.dim)</span><br><span class="line">        <span class="variable language_">self</span>.layers = nn.ModuleList(DCFormerBlock(config, lidx) <span class="keyword">for</span> lidx <span class="keyword">in</span> <span class="built_in">range</span>(config.n_layer))</span><br><span class="line">        <span class="variable language_">self</span>.norm = RMSNorm(config.dim, eps=config.norm_eps)</span><br><span class="line">        <span class="variable language_">self</span>.output = nn.Linear(config.dim, config.vocab_size, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.use_gradient_checkpointing = config.use_gradient_checkpointing </span><br><span class="line">        <span class="variable language_">self</span>.is_training = config.is_training</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.freqs_cis: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.mask_cache: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.window_size = config.window_size</span><br><span class="line">        <span class="variable language_">self</span>.max_batch_size = -<span class="number">1</span></span><br><span class="line">        <span class="variable language_">self</span>.max_seq_length = -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">setup_caches</span>(<span class="params">self, max_batch_size, max_seq_length, set_kv_cache=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.max_seq_length &gt;= max_seq_length <span class="keyword">and</span> <span class="variable language_">self</span>.max_batch_size &gt;= max_batch_size:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        head_dim = <span class="variable language_">self</span>.config.dim // <span class="variable language_">self</span>.config.n_head</span><br><span class="line">        max_seq_length = find_multiple(max_seq_length, <span class="number">8</span>)</span><br><span class="line">        <span class="variable language_">self</span>.max_seq_length = max_seq_length</span><br><span class="line">        <span class="variable language_">self</span>.max_batch_size = max_batch_size</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.is_training:</span><br><span class="line">            <span class="keyword">for</span> b <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line">                <span class="keyword">if</span> set_kv_cache:</span><br><span class="line">                    use_kw_cache = <span class="literal">False</span> <span class="keyword">if</span> b.attention.query_wise <span class="keyword">else</span> <span class="literal">True</span></span><br><span class="line">                    b.attention.kv_cache = KVKWCache(max_batch_size, max_seq_length, <span class="variable language_">self</span>.config.n_local_heads, head_dim, window_size=b.attention.window_size, use_kw_cache=use_kw_cache)</span><br><span class="line">                b.attention.dyn_w_proj.merge_weights()</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> b.attention.use_sw:</span><br><span class="line">                    dtype = b.attention.wo.weight.dtype</span><br><span class="line">                    device = b.attention.wo.weight.device</span><br><span class="line">                    b.attention.dyn_w_proj.sw = b.attention.dyn_w_proj.sw.to(device=device, dtype=dtype)</span><br><span class="line">                    b.attention.dyn_w_proj.pre_proj.w = b.attention.dyn_w_proj.pre_proj.w.to(device=device, dtype=dtype) </span><br><span class="line">                    b.attention.dyn_w_proj.post_proj.w = b.attention.dyn_w_proj.post_proj.w.to(device=device, dtype=dtype) </span><br><span class="line">                </span><br><span class="line">        <span class="variable language_">self</span>.freqs_cis = precompute_freqs_cis(<span class="variable language_">self</span>.config.block_size, <span class="variable language_">self</span>.config.dim // <span class="variable language_">self</span>.config.n_head, <span class="variable language_">self</span>.config.rope_base).to(<span class="variable language_">self</span>.tok_embeddings.weight.device)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.is_training:</span><br><span class="line">            <span class="variable language_">self</span>.causal_mask = torch.tril(torch.ones(<span class="variable language_">self</span>.config.block_size, <span class="variable language_">self</span>.config.block_size, dtype=torch.<span class="built_in">bool</span>, device=<span class="variable language_">self</span>.tok_embeddings.weight.device))</span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.window_size <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="variable language_">self</span>.causal_mask = torch.tril(torch.ones(max_seq_length, max_seq_length, dtype=torch.<span class="built_in">bool</span>, device=<span class="variable language_">self</span>.tok_embeddings.weight.device))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.causal_mask = torch.stack([make_window_mask(max_seq_length, <span class="variable language_">self</span>.config.window_size), torch.tril(torch.ones(<span class="variable language_">self</span>.max_seq_length, <span class="variable language_">self</span>.max_seq_length, dtype=torch.<span class="built_in">bool</span>))]) <span class="comment"># LG</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">self, input_ids, num_tokens_to_generate=<span class="number">10</span>, compiled_decode_one_token=<span class="literal">None</span></span>):</span><br><span class="line">        batch_size, seq_length = input_ids.shape</span><br><span class="line">        input_pos = torch.arange(seq_length, device=<span class="variable language_">self</span>.device)</span><br><span class="line">        generated_ids = torch.zeros(batch_size, seq_length + num_tokens_to_generate, dtype=torch.<span class="built_in">int</span>, device=<span class="variable language_">self</span>.device)</span><br><span class="line">        generated_ids[:, :seq_length] = input_ids.to(<span class="variable language_">self</span>.device).to(torch.<span class="built_in">int</span>)</span><br><span class="line">        logits = <span class="variable language_">self</span>.forward(input_ids, input_pos=input_pos,return_tensor=<span class="literal">True</span>)</span><br><span class="line">        _next_token = torch.argmax(logits[:, -<span class="number">1</span>], dim=-<span class="number">1</span>)[:, <span class="literal">None</span>]</span><br><span class="line">        next_token = torch.zeros(<span class="variable language_">self</span>.max_batch_size, <span class="number">1</span>, device=<span class="variable language_">self</span>.device, dtype=torch.<span class="built_in">int</span>)</span><br><span class="line">        next_token[:batch_size] = _next_token</span><br><span class="line">        generated_ids[:, seq_length] = next_token[:batch_size, <span class="number">0</span>]</span><br><span class="line">        input_pos = torch.tensor([seq_length], device=<span class="variable language_">self</span>.device)</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num_tokens_to_generate):</span><br><span class="line">            <span class="keyword">if</span> compiled_decode_one_token <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                next_token = compiled_decode_one_token(<span class="variable language_">self</span>, next_token.clone(), input_pos)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                next_token = <span class="variable language_">self</span>.decode_one_token(next_token.clone(), input_pos)</span><br><span class="line">            generated_ids[:, input_pos+<span class="number">1</span>] = next_token.<span class="built_in">int</span>()[:batch_size]</span><br><span class="line">            input_pos += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> generated_ids</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode_one_token</span>(<span class="params">self, cur_token, input_pos</span>):</span><br><span class="line">        logits = <span class="variable language_">self</span>.forward(</span><br><span class="line">            cur_token,</span><br><span class="line">            input_pos=input_pos,</span><br><span class="line">            return_tensor=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">        new_token = torch.argmax(logits[:, -<span class="number">1</span>], dim=-<span class="number">1</span>)[:,<span class="literal">None</span>]</span><br><span class="line">        <span class="keyword">return</span> new_token</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, idx: Tensor, input_pos: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, return_tensor=<span class="literal">False</span></span>) -&gt; Tensor:</span><br><span class="line">        <span class="keyword">assert</span> <span class="variable language_">self</span>.freqs_cis <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>, <span class="string">&quot;Caches must be initialized first&quot;</span></span><br><span class="line">        <span class="keyword">if</span> input_pos <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            input_pos = torch.arange(idx.shape[-<span class="number">1</span>], device=idx.device, dtype=torch.<span class="built_in">int</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.window_size <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> <span class="variable language_">self</span>.is_training:</span><br><span class="line">            mask = <span class="variable language_">self</span>.causal_mask[<span class="literal">None</span>, <span class="literal">None</span>, input_pos]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mask = <span class="variable language_">self</span>.causal_mask[<span class="literal">None</span>, <span class="literal">None</span>,:,input_pos]</span><br><span class="line">        freqs_cis = <span class="variable language_">self</span>.freqs_cis[input_pos][:idx.shape[-<span class="number">1</span>]]</span><br><span class="line">        x = <span class="variable language_">self</span>.tok_embeddings(idx)</span><br><span class="line">        <span class="keyword">for</span> i, layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.layers):</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.is_training <span class="keyword">or</span> <span class="variable language_">self</span>.window_size <span class="keyword">is</span> <span class="literal">None</span> :</span><br><span class="line">                layer_mask = mask</span><br><span class="line">                gen_mask = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">elif</span> <span class="variable language_">self</span>.window_size <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: </span><br><span class="line">                layer_mask = mask[:,:,<span class="number">1</span>] <span class="keyword">if</span> layer.attention.window_size <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> mask[:,:,<span class="number">0</span>]</span><br><span class="line">                gen_mask = mask[:,:,<span class="number">1</span>] <span class="keyword">if</span> layer.attention.window_size <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span> </span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.use_gradient_checkpointing:</span><br><span class="line">                x = checkpoint(layer, x, input_pos, freqs_cis, layer_mask)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                x = layer(x, input_pos, freqs_cis, layer_mask, gen_mask=gen_mask)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm(x)</span><br><span class="line">        logits = <span class="variable language_">self</span>.output(x)</span><br><span class="line">        <span class="keyword">if</span> return_tensor:</span><br><span class="line">            <span class="keyword">return</span> logits</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            CausalLMOutput = namedtuple(<span class="string">&quot;CausalLMOutput&quot;</span>, [<span class="string">&quot;logits&quot;</span>])</span><br><span class="line">            <span class="keyword">return</span> CausalLMOutput(logits=logits)</span><br></pre></td></tr></table></figure>

<p>这是模型的核心类，继承自 PreTrainedModel，具备加载和保存模型的能力。</p>
<p><strong>结构:</strong><br>-词嵌入层（tok_embeddings）。<br>-多个 Transformer 块（DCFormerBlock）。<br>-RMSNorm 归一化和输出层。</p>
<p><strong>方法解析:</strong><br>-setup_caches:<br>初始化 KV 缓存。<br>预计算频率编码（freqs_cis）用于旋转位置编码。<br>-generate:<br>循环生成文本，支持快速解码（compiled_decode_one_token）。<br>-forward:<br>依次通过各层 Transformer 块。<br>支持梯度检查点（节省显存）。<br>根据窗口类型和训练模式切换掩码。</p>
<p><strong>特点:</strong><br>使用 RMSNorm 替代 LayerNorm。<br>通过 KV 缓存和窗口机制优化推理。</p>
<h4 id="1-5-3-DCFormerBlock"><a href="#1-5-3-DCFormerBlock" class="headerlink" title="1.5.3 DCFormerBlock"></a>1.5.3 DCFormerBlock</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DCFormerBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config: DCFormerConfig, lidx</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.lidx = lidx</span><br><span class="line">        <span class="variable language_">self</span>.attention = DCMHAttention(config, lidx)</span><br><span class="line">        <span class="variable language_">self</span>.feed_forward = FeedForward(config)</span><br><span class="line">        <span class="variable language_">self</span>.ffn_norm = RMSNorm(config.dim, config.norm_eps)</span><br><span class="line">        <span class="variable language_">self</span>.attention_norm = RMSNorm(config.dim, config.norm_eps)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor, input_pos: Tensor, freqs_cis: Tensor, mask: Tensor, gen_mask=<span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">        h = x + <span class="variable language_">self</span>.attention(<span class="variable language_">self</span>.attention_norm(x), freqs_cis, mask, input_pos, gen_mask=gen_mask, fast_infer=<span class="literal">True</span>)</span><br><span class="line">        out = h + <span class="variable language_">self</span>.feed_forward(<span class="variable language_">self</span>.ffn_norm(h))</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

<p>单个 Transformer 块，包含以下模块：<br>-动态加权多头注意力（DCMHAttention）。<br>-前馈网络（FeedForward）。<br>-两个归一化层（RMSNorm）。</p>
<p><strong>实现:</strong><br>注意力和前馈网络均通过残差连接（h &#x3D; x + …）相加。</p>
<h4 id="1-5-4-DCMHAttention"><a href="#1-5-4-DCMHAttention" class="headerlink" title="1.5.4 DCMHAttention"></a>1.5.4 DCMHAttention</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DCMHAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config: DCFormerConfig, lidx, use_sw=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">assert</span> config.dim % config.n_head == <span class="number">0</span></span><br><span class="line">        total_head_dim = (config.n_head + <span class="number">2</span> * config.n_local_heads) * config.head_dim</span><br><span class="line">        <span class="comment"># key, query, value projections for all heads, but in a batch</span></span><br><span class="line">        <span class="variable language_">self</span>.lidx = lidx</span><br><span class="line">        <span class="variable language_">self</span>.wqkv = nn.Linear(config.dim, total_head_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.wo = nn.Linear(config.dim, config.dim, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.kv_cache = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.n_head = config.n_head</span><br><span class="line">        <span class="variable language_">self</span>.head_dim = config.head_dim</span><br><span class="line">        <span class="variable language_">self</span>.n_local_heads = config.n_local_heads</span><br><span class="line">        <span class="variable language_">self</span>.is_training = config.is_training</span><br><span class="line">        <span class="variable language_">self</span>.dim = config.dim</span><br><span class="line">        <span class="variable language_">self</span>.use_dcmha = config.use_dcmha </span><br><span class="line">        <span class="variable language_">self</span>.scale_factor = <span class="number">1</span> / math.sqrt(<span class="variable language_">self</span>.head_dim)</span><br><span class="line">        <span class="variable language_">self</span>.q_chunk_size = config.q_chunk_size </span><br><span class="line">        <span class="variable language_">self</span>.use_sw = use_sw </span><br><span class="line">        <span class="variable language_">self</span>.dyn_w_proj = DynamicWeightProjection(num_heads=<span class="variable language_">self</span>.n_head, query_input_dim=config.dim, dynamic_squeeze_ratio=<span class="variable language_">self</span>.n_head//<span class="number">2</span>, dynamic_w_hidden_dim=<span class="variable language_">self</span>.n_head*<span class="number">4</span>, use_sw=use_sw)</span><br><span class="line">        <span class="variable language_">self</span>.use_qk_norm = config.use_qk_norm </span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_qk_norm:</span><br><span class="line">            <span class="variable language_">self</span>.q_norm = RMSnorm(hid_dim=<span class="variable language_">self</span>.head_dim)</span><br><span class="line">            <span class="variable language_">self</span>.k_norm = RMSnorm(hid_dim=<span class="variable language_">self</span>.head_dim)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.window_types = &#123;</span><br><span class="line">            <span class="string">&quot;LG&quot;</span>:[<span class="number">256</span>, <span class="literal">None</span>],</span><br><span class="line">            <span class="string">&quot;LGLL&quot;</span>:[<span class="number">256</span>, <span class="literal">None</span>, <span class="number">256</span>, <span class="number">256</span>],</span><br><span class="line">            <span class="string">&quot;LGL6&quot;</span>:[<span class="number">256</span>, <span class="literal">None</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>],</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.query_wise = config.query_wise</span><br><span class="line">        <span class="keyword">if</span> config.window_type <span class="keyword">is</span> <span class="literal">None</span>: <span class="comment"># LG </span></span><br><span class="line">            <span class="variable language_">self</span>.window_size = <span class="literal">None</span> <span class="keyword">if</span> <span class="variable language_">self</span>.lidx % <span class="number">2</span> == <span class="number">1</span> <span class="keyword">else</span> config.window_size </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            window_l = <span class="variable language_">self</span>.window_types[config.window_type]</span><br><span class="line">            <span class="variable language_">self</span>.window_size = window_l[<span class="variable language_">self</span>.lidx % <span class="built_in">len</span>(window_l)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.is_training:</span><br><span class="line">            <span class="variable language_">self</span>._register_load_state_dict_pre_hook(<span class="variable language_">self</span>.load_hook)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load_hook</span>(<span class="params">self, state_dict, prefix, *args</span>):</span><br><span class="line">        <span class="keyword">if</span> prefix + <span class="string">&quot;wq.weight&quot;</span> <span class="keyword">in</span> state_dict:</span><br><span class="line">            wq = state_dict.pop(prefix + <span class="string">&quot;wq.weight&quot;</span>)</span><br><span class="line">            wk = state_dict.pop(prefix + <span class="string">&quot;wk.weight&quot;</span>)</span><br><span class="line">            wv = state_dict.pop(prefix + <span class="string">&quot;wv.weight&quot;</span>)</span><br><span class="line">            state_dict[prefix + <span class="string">&quot;wqkv.weight&quot;</span>] = torch.cat([wq, wk, wv])</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_generate_fast</span>(<span class="params">self, x, input_pos, q, k, v, k_mask</span>):</span><br><span class="line">        B,T,D = x.shape</span><br><span class="line">        N,I = <span class="variable language_">self</span>.n_head, <span class="variable language_">self</span>.dyn_w_proj.dynamic_hidden_dim <span class="comment"># 32, 2</span></span><br><span class="line">        dw_hidden, dd = (x @ <span class="variable language_">self</span>.dyn_w_proj.dw_m).split([<span class="number">2</span>*<span class="number">2</span>*N*(<span class="number">2</span>*I), <span class="number">2</span>*<span class="number">2</span>*N*<span class="number">1</span>], -<span class="number">1</span>) <span class="comment"># BTD, D(4K+4N) -&gt; BT(4K+4N) -&gt; BT(4K), BT(4N)</span></span><br><span class="line">        dw_hidden = dw_hidden.view((B,T,<span class="number">4</span>,-<span class="number">1</span>,<span class="number">1</span>)) <span class="comment"># BT(4K) -&gt; BT4K1</span></span><br><span class="line">        dw = (<span class="variable language_">self</span>.dyn_w_proj.dw_hidden_activation(dw_hidden) * <span class="variable language_">self</span>.dyn_w_proj.qkw_m).<span class="built_in">sum</span>(-<span class="number">2</span>) <span class="comment"># gelu, BT4K1, 4K(IM)-&gt;BT4K(IM)-&gt;BT4(IM)</span></span><br><span class="line">        w1, w2 = dw.view((B,T,<span class="number">2</span>,<span class="number">2</span>,-<span class="number">1</span>,N)).split(I,-<span class="number">2</span>) <span class="comment"># BT4(IM)-&gt;BT&#123;pre/post&#125;&#123;q/k&#125;IM-&gt;[BT22IM] * 2</span></span><br><span class="line">        w1 = <span class="variable language_">self</span>.dyn_w_proj.dw1_norm(w1) <span class="comment"># BT22IN</span></span><br><span class="line">        qkdd = <span class="variable language_">self</span>.dyn_w_proj.dw_activation(dd.view((B,T,<span class="number">2</span>,<span class="number">2</span>,N))) <span class="comment"># BT2&#123;2&#125;N1-&gt;BT2&#123;2&#125;N tanh</span></span><br><span class="line">        qkw = torch.einsum(<span class="string">&#x27;BTKJIN,BTKJIM-&gt;BTKJNM&#x27;</span>, w1, w2) + torch.diag_embed(qkdd) <span class="comment"># j=k=2, BT2&#123;2&#125;NM q/k, pre/post</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.query_wise: <span class="comment"># <span class="doctag">TODO:</span> do not generate kw and kdd</span></span><br><span class="line">            qw, _ = qkw.unbind(<span class="number">3</span>) <span class="comment"># BS2NM</span></span><br><span class="line">            kw_new = <span class="literal">None</span></span><br><span class="line">            qw = qw + <span class="variable language_">self</span>.dyn_w_proj.sw </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            qw, kw_new = qkw.unbind(<span class="number">3</span>) <span class="comment"># BS&#123;pre/post&#125;&#123;q/k&#125;NM -&gt; BS&#123;pre/post&#125;NM * 2</span></span><br><span class="line">            kw_new = kw_new + <span class="variable language_">self</span>.dyn_w_proj.sw  <span class="comment"># BS2NM + 2NM-&gt; BS2NM </span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.kv_cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            k, v, kw_out = <span class="variable language_">self</span>.kv_cache.update(input_pos, k, v, kw_val=kw_new) <span class="comment">#BNT2M</span></span><br><span class="line">        logits = q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>) * <span class="variable language_">self</span>.scale_factor </span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.query_wise:</span><br><span class="line">            w = qw  <span class="comment"># B12NM</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            w = qw + kw_out <span class="comment"># B12NM,BS2NM -&gt; BS2NM </span></span><br><span class="line">        wl, w = w.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>).unbind(<span class="number">1</span>)  <span class="comment"># BS2NM-&gt;B2NMS-&gt;[BNMS]*2 </span></span><br><span class="line">        logits = (logits * wl).<span class="built_in">sum</span>(<span class="number">1</span>).unsqueeze(<span class="number">2</span>) <span class="comment"># BN1S, BNMS -&gt; BNMS-&gt; BMS-&gt; BM1S </span></span><br><span class="line">        min_value = torch.finfo(torch.float16).<span class="built_in">min</span></span><br><span class="line">        logits = torch.where(k_mask, logits, min_value)</span><br><span class="line">        probs = logits.softmax(-<span class="number">1</span>)</span><br><span class="line">        probs = (probs * w).<span class="built_in">sum</span>(<span class="number">1</span>).unsqueeze(<span class="number">2</span>)</span><br><span class="line">        y = probs @ v</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor, freqs_cis: Tensor, mask: Tensor, input_pos: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, fast_infer=<span class="literal">True</span>, gen_mask=<span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">        bsz, seqlen, _ = x.shape</span><br><span class="line"></span><br><span class="line">        kv_size = <span class="variable language_">self</span>.n_local_heads * <span class="variable language_">self</span>.head_dim</span><br><span class="line">        q, k, v = <span class="variable language_">self</span>.wqkv(x).split([<span class="variable language_">self</span>.dim, kv_size, kv_size], dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        q = q.view(bsz, seqlen, <span class="variable language_">self</span>.n_head, <span class="variable language_">self</span>.head_dim) <span class="comment"># BSND</span></span><br><span class="line">        k = k.view(bsz, seqlen, <span class="variable language_">self</span>.n_local_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line">        v = v.view(bsz, seqlen, <span class="variable language_">self</span>.n_local_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_qk_norm:</span><br><span class="line">            q, k = <span class="variable language_">self</span>.q_norm(q), <span class="variable language_">self</span>.k_norm(k)</span><br><span class="line"></span><br><span class="line">        q = apply_rotary_emb(q, freqs_cis)</span><br><span class="line">        k = apply_rotary_emb(k, freqs_cis)</span><br><span class="line"></span><br><span class="line">        q, k, v = <span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.transpose(<span class="number">1</span>, <span class="number">2</span>), (q, k, v)) <span class="comment"># BNSD</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.is_training:</span><br><span class="line">            N, D, I = <span class="variable language_">self</span>.n_head, <span class="variable language_">self</span>.head_dim, <span class="variable language_">self</span>.dyn_w_proj.dynamic_hidden_dim; <span class="comment"># 6.7B</span></span><br><span class="line">            B,T,E = x.shape</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.use_dcmha:</span><br><span class="line">                project_logits = <span class="literal">True</span> </span><br><span class="line">                project_probs = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">if</span> project_probs:</span><br><span class="line">                    dw_hidden, dd = (x @ <span class="variable language_">self</span>.dyn_w_proj.dw_m).split([<span class="number">2</span>*<span class="number">2</span>*N*(<span class="number">2</span>*I), <span class="number">2</span>*<span class="number">2</span>*N*<span class="number">1</span>], -<span class="number">1</span>)</span><br><span class="line">                    dw_hidden = <span class="variable language_">self</span>.dyn_w_proj.dw_hidden_activation(dw_hidden) </span><br><span class="line">                    dw_hidden = dw_hidden.view(dw_hidden.shape[:<span class="number">2</span>]+(<span class="number">4</span>,-<span class="number">1</span>)) <span class="comment">#B T (4 K) -&gt; B T 4 K  # reshape</span></span><br><span class="line">                    dw = torch.einsum(<span class="string">&#x27;B T C K, C K D -&gt; B T C D&#x27;</span>, dw_hidden, <span class="variable language_">self</span>.dyn_w_proj.qkw_m) <span class="comment"># BT4K,4K(MI)-&gt;BT4(MI)</span></span><br><span class="line">                    shape = (B,T,<span class="number">2</span>*<span class="number">2</span>,-<span class="number">1</span>,N)<span class="comment"># if project_logits else (B,T,2,N,-1)  # BT(pre/post)(q/k)IN</span></span><br><span class="line">                    w1, w2 = dw.view(shape).split(I,-<span class="number">2</span>)</span><br><span class="line">                    w1 = <span class="variable language_">self</span>.dyn_w_proj.dw1_norm(w1) <span class="comment"># BT22IN</span></span><br><span class="line">                    <span class="keyword">if</span> <span class="variable language_">self</span>.use_sw:</span><br><span class="line">                        pre_sw, post_sw = <span class="variable language_">self</span>.dyn_w_proj.sw.unbind(<span class="number">0</span>)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        pre_sw, post_sw = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">                    pre_qw1, pre_kw1, post_qw1, post_kw1 = w1.unbind(<span class="number">2</span>)  <span class="comment"># BT(2&#123;*2&#125;)IN-&gt;[BTIN]*4</span></span><br><span class="line">                    pre_qw2, pre_kw2, post_qw2, post_kw2 = w2.unbind(<span class="number">2</span>)</span><br><span class="line">                    qkdd = F.tanh(dd).squeeze(-<span class="number">1</span>).view(shape[:-<span class="number">2</span>] + (N,)) <span class="comment"># BT(2&#123;*2&#125;)N1-&gt;BT(2&#123;*2&#125;)N</span></span><br><span class="line">                    pre_qdd, pre_kdd, post_qdd, post_kdd = qkdd.unbind(<span class="number">2</span>)  <span class="comment"># BT(2&#123;*2&#125;)N-&gt;[BTN]*4</span></span><br><span class="line"></span><br><span class="line">                y = torch.zeros(B, N, T, D).to(q.device, dtype=torch.float16)</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(T // <span class="variable language_">self</span>.q_chunk_size):</span><br><span class="line">                    start, stop = i * <span class="variable language_">self</span>.q_chunk_size, (i + <span class="number">1</span>) * <span class="variable language_">self</span>.q_chunk_size</span><br><span class="line">                    kv_start = <span class="built_in">max</span>(<span class="number">0</span>, stop - <span class="variable language_">self</span>.q_chunk_size -<span class="variable language_">self</span>.window_size)</span><br><span class="line">                    _q = q[:, :, start : stop, :]</span><br><span class="line">                    _k, _v = k[:, :, kv_start : stop, :], v[:, :, kv_start : stop, :]</span><br><span class="line">                    _atten_mask = mask[:, :, start : stop, kv_start : stop]</span><br><span class="line">                    _pre_proj_dw_args = slice_dw(pre_sw, pre_qw1, pre_qw2, pre_kw1, pre_kw2, pre_qdd, pre_kdd, start, stop, kv_start) \</span><br><span class="line">                        <span class="keyword">if</span> project_logits <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">                    _post_proj_dw_args = slice_dw(post_sw, post_qw1, post_qw2, post_kw1, post_kw2, post_qdd, post_kdd, start,stop,kv_start) \</span><br><span class="line">                        <span class="keyword">if</span> project_probs <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">                    _o = _atten_context(_q, _k, _v, _atten_mask, _pre_proj_dw_args, _post_proj_dw_args)</span><br><span class="line">                    y[:,:,start:stop] = _o</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                y = torch.zeros(B, N, T, D).to(q.device, dtype=torch.float16)</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(T // <span class="variable language_">self</span>.q_chunk_size):</span><br><span class="line">                    start, stop = i * <span class="variable language_">self</span>.q_chunk_size, (i + <span class="number">1</span>) * <span class="variable language_">self</span>.q_chunk_size</span><br><span class="line">                    kv_start = <span class="built_in">max</span>(<span class="number">0</span>, stop - <span class="variable language_">self</span>.q_chunk_size -<span class="variable language_">self</span>.window_size)</span><br><span class="line">                    _q = q[:, :, start : stop, :]</span><br><span class="line">                    _k, _v = k[:, :, kv_start : stop, :], v[:, :, kv_start : stop, :]</span><br><span class="line">                    _atten_mask = mask[:, :, start : stop, kv_start : stop]</span><br><span class="line">                    _pre_proj_dw_args, _post_proj_dw_args = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">                    _o = _atten_context(_q, _k, _v, _atten_mask, _pre_proj_dw_args, _post_proj_dw_args)</span><br><span class="line">                    y[:,:,start:stop] = _o</span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># inference</span></span><br><span class="line">            <span class="keyword">if</span> seqlen == <span class="number">1</span>: <span class="comment"># one-token generation</span></span><br><span class="line">                k_mask = mask <span class="keyword">if</span> <span class="variable language_">self</span>.window_size <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> gen_mask[:, :, :,:<span class="variable language_">self</span>.kv_cache.seq_length] </span><br><span class="line">                <span class="keyword">if</span> fast_infer:</span><br><span class="line">                    y = <span class="variable language_">self</span>._generate_fast(x, input_pos, q, k, v, k_mask)</span><br><span class="line">                <span class="keyword">else</span>: </span><br><span class="line">                    <span class="keyword">assert</span> <span class="keyword">not</span> <span class="variable language_">self</span>.query_wise</span><br><span class="line">                    <span class="comment"># generate dw from hidden_state</span></span><br><span class="line">                    pre_proj_dw_args, post_proj_dw_args, kw_new = <span class="variable language_">self</span>.dyn_w_proj(x, gen_cache=<span class="literal">True</span>)</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># update kvkw cache</span></span><br><span class="line">                    kw_new = kw_new + <span class="variable language_">self</span>.dyn_w_proj.sw <span class="comment"># absorb residual or sw into kw cache</span></span><br><span class="line">                    <span class="keyword">if</span> <span class="variable language_">self</span>.kv_cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                        k, v, kw_out = <span class="variable language_">self</span>.kv_cache.update(input_pos, k, v, kw_val=kw_new) <span class="comment"># BNSD, BNSD, BS2NN</span></span><br><span class="line"></span><br><span class="line">                    logits = q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>) * <span class="variable language_">self</span>.scale_factor</span><br><span class="line">                    <span class="comment"># merge pre_w and apply it</span></span><br><span class="line">                    pre_qw1, pre_qw2, pre_kw1, pre_kw2, pre_qdd, pre_kdd = pre_proj_dw_args</span><br><span class="line">                    pre_qw = torch.einsum(<span class="string">&#x27;BTGIN, BTGIM-&gt;BTNM&#x27;</span>,pre_qw1, pre_qw2)  + torch.diag_embed(pre_qdd.squeeze(<span class="number">2</span>))</span><br><span class="line">                    pre_w = pre_qw + kw_out[:,:,<span class="number">0</span>] <span class="comment"># B1NM, BSNM -&gt; BSNM</span></span><br><span class="line">                    logits = <span class="variable language_">self</span>.dyn_w_proj.pre_proj(logits, proj_w=pre_w.squeeze(<span class="number">1</span>))</span><br><span class="line">  </span><br><span class="line">                    logits = torch.where(k_mask, logits, torch.finfo(torch.float16).<span class="built_in">min</span>)</span><br><span class="line">                    probs = logits.softmax(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># merge post_w and apply it</span></span><br><span class="line">                    post_qw1, post_qw2, post_kw1, post_kw2, post_qdd, post_kdd = post_proj_dw_args</span><br><span class="line">                    post_qw = torch.einsum(<span class="string">&#x27;BTGIN, BTGIM-&gt;BTNM&#x27;</span>, post_qw1, post_qw2) + torch.diag_embed(post_qdd.squeeze(<span class="number">2</span>))</span><br><span class="line">                    post_w = post_qw + kw_out[:,:,<span class="number">1</span>]</span><br><span class="line">                    probs = <span class="variable language_">self</span>.dyn_w_proj.post_proj(probs, proj_w=post_w.squeeze(<span class="number">1</span>)) </span><br><span class="line"></span><br><span class="line">                    y = probs @ v                  </span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># prefill</span></span><br><span class="line">                k_mask = mask[:,:,:,:k.shape[-<span class="number">2</span>]] </span><br><span class="line">                pre_proj_dw_args, post_proj_dw_args,kw_new = <span class="variable language_">self</span>.dyn_w_proj(x, gen_cache=<span class="literal">True</span>)</span><br><span class="line">                kw_new = kw_new + <span class="variable language_">self</span>.dyn_w_proj.sw <span class="comment"># absorb residual or sw into kw cache</span></span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.kv_cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="variable language_">self</span>.kv_cache.update(input_pos, k, v, kw_val=kw_new) <span class="comment"># BNSD, BNSD, BS2NN</span></span><br><span class="line">                logits = q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>) * <span class="variable language_">self</span>.scale_factor </span><br><span class="line">                logits = <span class="variable language_">self</span>.dyn_w_proj.pre_proj(logits, dws=pre_proj_dw_args, query_vec=x, key_vec=x, fast_infer=<span class="literal">True</span>)  <span class="comment"># XD BN1S</span></span><br><span class="line">                logits = torch.where(k_mask, logits, torch.finfo(torch.float16).<span class="built_in">min</span>)</span><br><span class="line">                probs = logits.softmax(-<span class="number">1</span>)</span><br><span class="line">                probs = <span class="variable language_">self</span>.dyn_w_proj.post_proj(probs, dws=post_proj_dw_args, query_vec=x, key_vec=x, fast_infer=<span class="literal">True</span>) <span class="comment"># BN1S</span></span><br><span class="line">                y = probs @ v</span><br><span class="line"></span><br><span class="line">        y = y.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(bsz, seqlen, <span class="variable language_">self</span>.dim)</span><br><span class="line">        y = <span class="variable language_">self</span>.wo(y)</span><br><span class="line">        <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>

<p>本部分为动态加权多头注意力模块，是模型的核心改进。</p>
<p><strong>功能:</strong><br>计算 Query、Key 和 Value（通过 wqkv 投影）。<br>支持 Query 和 Key 的归一化。<br>根据窗口类型和位置掩码调整计算范围。<br>使用 DynamicWeightProjection 动态调整注意力权重。</p>
<p><strong>推理优化:</strong><br>_generate_fast 用于快速单步解码。<br>支持按块分组（q_chunk_size）计算，减少内存开销。</p>
<p><strong>动态权重投影的关键逻辑:</strong><br>通过动态权重投影（DynamicWeightProjection）生成权重。<br>fast_infer 模式下，利用缓存加速计算。</p>
<h4 id="1-5-5-DynamicWeightProjection"><a href="#1-5-5-DynamicWeightProjection" class="headerlink" title="1.5.5 DynamicWeightProjection"></a>1.5.5 DynamicWeightProjection</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DynamicWeightProjection</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_heads=<span class="number">32</span>, num_groups=<span class="number">1</span>, residual=<span class="literal">True</span>, query_input_dim=<span class="number">4096</span>, dynamic_squeeze_ratio=<span class="number">16</span>, dynamic_w_hidden_dim=<span class="number">128</span>,dtype=torch.float16,use_sw=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads </span><br><span class="line">        <span class="variable language_">self</span>.num_groups = num_groups </span><br><span class="line">        <span class="variable language_">self</span>.query_input_dim = query_input_dim </span><br><span class="line">        <span class="variable language_">self</span>.dynamic_squeeze_ratio = dynamic_squeeze_ratio</span><br><span class="line">        <span class="variable language_">self</span>.dynamic_w_hidden_dim = dynamic_w_hidden_dim </span><br><span class="line">        <span class="variable language_">self</span>.dw_hidden_activation = nn.GELU()</span><br><span class="line">        <span class="variable language_">self</span>.num_heads_per_group = <span class="variable language_">self</span>.num_heads // <span class="variable language_">self</span>.num_groups</span><br><span class="line">        <span class="variable language_">self</span>.dw_activation = nn.Tanh()</span><br><span class="line">        <span class="variable language_">self</span>.dw1_norm = RMSnormNoscale(dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.use_sw = use_sw</span><br><span class="line">        <span class="variable language_">self</span>.pre_proj = CrossHeadProjection(<span class="string">&#x27;pre&#x27;</span>, num_heads=<span class="variable language_">self</span>.num_heads, use_sw=use_sw)</span><br><span class="line">        <span class="variable language_">self</span>.post_proj = CrossHeadProjection(<span class="string">&#x27;post&#x27;</span>, num_heads=<span class="variable language_">self</span>.num_heads, use_sw=use_sw)</span><br><span class="line"></span><br><span class="line">        dynamic_hidden_dim = <span class="variable language_">self</span>.num_heads_per_group // <span class="variable language_">self</span>.dynamic_squeeze_ratio </span><br><span class="line">        <span class="variable language_">self</span>.dynamic_hidden_dim = dynamic_hidden_dim </span><br><span class="line">        <span class="variable language_">self</span>.dw1 = nn.parameter.Parameter(torch.zeros(<span class="variable language_">self</span>.query_input_dim, <span class="variable language_">self</span>.num_groups, <span class="number">4</span>, <span class="variable language_">self</span>.dynamic_w_hidden_dim, dtype=dtype)) <span class="comment">#(4096, 1, 4, 128)</span></span><br><span class="line">        G, K, M = <span class="variable language_">self</span>.num_groups, <span class="variable language_">self</span>.dynamic_w_hidden_dim, <span class="variable language_">self</span>.num_heads_per_group</span><br><span class="line">        I = dynamic_hidden_dim * <span class="number">2</span> </span><br><span class="line">        <span class="variable language_">self</span>.qkw = nn.parameter.Parameter(torch.zeros([G, <span class="number">4</span>, K, I, M], dtype=dtype)) <span class="comment"># (1, 4, 128, 4, 32)</span></span><br><span class="line">        <span class="variable language_">self</span>.dd = nn.parameter.Parameter(torch.zeros(<span class="variable language_">self</span>.query_input_dim, <span class="variable language_">self</span>.num_groups, <span class="variable language_">self</span>.num_heads_per_group * <span class="number">4</span>, dtype=dtype)) <span class="comment">#  (4096, 1, 128)</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.merge_weights()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">merge_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.dw_m = nn.parameter.Parameter(torch.cat([<span class="variable language_">self</span>.dw1.reshape(<span class="variable language_">self</span>.query_input_dim, -<span class="number">1</span>), <span class="variable language_">self</span>.dd.squeeze(<span class="number">1</span>)], dim=-<span class="number">1</span>)).to(<span class="variable language_">self</span>.dw1.device) <span class="comment"># E,(4*K + K)  K=2*N*I</span></span><br><span class="line">        <span class="variable language_">self</span>.qkw_m = nn.parameter.Parameter(<span class="variable language_">self</span>.qkw.permute(<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>).reshape(<span class="number">4</span>,<span class="variable language_">self</span>.dynamic_w_hidden_dim,-<span class="number">1</span>)).to(<span class="variable language_">self</span>.dw1.device) <span class="comment">#(4,K,I*M)</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_sw:</span><br><span class="line">            <span class="variable language_">self</span>.sw = nn.parameter.Parameter(torch.stack([<span class="variable language_">self</span>.pre_proj.w, <span class="variable language_">self</span>.post_proj.w]).squeeze(<span class="number">1</span>) + torch.eye(<span class="variable language_">self</span>.num_heads) ).to(<span class="variable language_">self</span>.dw1.device) <span class="comment"># (2,N,N) sw + identity matrix</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.sw = (torch.eye(<span class="variable language_">self</span>.num_heads).expand(<span class="number">2</span>,<span class="variable language_">self</span>.num_heads,<span class="variable language_">self</span>.num_heads)).to(<span class="variable language_">self</span>.dw1.device) <span class="comment"># identity matrix (2,N,N)</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,query_vec,KW:<span class="type">Optional</span>[torch.Tensor]=<span class="literal">None</span>, gen_cache:<span class="type">Optional</span>[<span class="built_in">bool</span>]=<span class="literal">True</span></span>):  </span><br><span class="line">        dw_hidden = torch.einsum(<span class="string">&#x27;BTD,DGCK-&gt;BTGCK&#x27;</span>, query_vec, <span class="variable language_">self</span>.dw1)  <span class="comment"># C=4 [pre,post]*[query,key]</span></span><br><span class="line">        dw_hidden = <span class="variable language_">self</span>.dw_hidden_activation(dw_hidden) <span class="comment">#BTGCK</span></span><br><span class="line">        w1, w2 = torch.split(torch.einsum(<span class="string">&#x27;BTGCK,GCKIM-&gt;BTGCIM&#x27;</span>, dw_hidden, <span class="variable language_">self</span>.qkw), <span class="variable language_">self</span>.qkw.shape[-<span class="number">2</span>]//<span class="number">2</span>, dim=-<span class="number">2</span>) <span class="comment">#BTGC(2I)M -&gt; [BTGCIM] * 2</span></span><br><span class="line">        w1 = <span class="variable language_">self</span>.dw1_norm(w1) <span class="comment"># BTGCIM</span></span><br><span class="line">        pre_qw1, pre_kw1, post_qw1, post_kw1 = unbind(w1, <span class="number">4</span>, dim=<span class="number">3</span>) <span class="comment"># BTG4IM-&gt;[BTGIM]*4</span></span><br><span class="line">        pre_qw2, pre_kw2, post_qw2, post_kw2 = unbind(w2, <span class="number">4</span>, dim=<span class="number">3</span>) </span><br><span class="line">        dd = torch.einsum(<span class="string">&#x27;BTD,DGM-&gt;BTGM&#x27;</span>, query_vec, <span class="variable language_">self</span>.dd) <span class="comment"># BTG(4M)</span></span><br><span class="line">        dd = <span class="variable language_">self</span>.dw_activation(dd)</span><br><span class="line">        pre_qdd, pre_kdd, post_qdd, post_kdd = torch.split(dd, dd.shape[-<span class="number">1</span>] // <span class="number">4</span>, dim=-<span class="number">1</span>) <span class="comment"># BTG(4N)-&gt;[BTGN]*4</span></span><br><span class="line">        pre_dw_args = (pre_qw1, pre_qw2, pre_kw1, pre_kw2, pre_qdd, pre_kdd)</span><br><span class="line">        post_dw_args = (post_qw1, post_qw2, post_kw1, post_kw2, post_qdd, post_kdd)</span><br><span class="line">        <span class="keyword">if</span> gen_cache: <span class="comment"># generate KW cache</span></span><br><span class="line">            pre_kw = torch.einsum(<span class="string">&#x27;BSGIM, BSGIN-&gt;BSMN&#x27;</span>, pre_kw1, pre_kw2) + torch.diag_embed(pre_kdd.squeeze(<span class="number">2</span>))  <span class="comment"># merge kw and kdd</span></span><br><span class="line">            post_kw = torch.einsum(<span class="string">&#x27;BSGIM, BSGIN-&gt;BSMN&#x27;</span>, post_kw1, post_kw2) + torch.diag_embed(post_kdd.squeeze(<span class="number">2</span>))</span><br><span class="line">            KW = torch.stack((pre_kw, post_kw), dim=-<span class="number">3</span>) <span class="comment"># BSMN,BSMN-&gt;BS2MN</span></span><br><span class="line">        <span class="keyword">return</span> pre_dw_args, post_dw_args, KW</span><br></pre></td></tr></table></figure>

<p>实现动态权重投影，为 Query 和 Key 生成自适应权重。</p>
<p><strong>特点:</strong><br>动态生成投影权重 (dw_hidden, qkw)。<br>支持分组权重（num_groups）。<br>提供预投影（pre_proj）和后投影（post_proj）的支持。</p>
<p><strong>关键方法:</strong><br>merge_weights: 将预定义权重合并，用于优化推理。<br>forward: 根据输入动态生成权重，并选择是否缓存。</p>
<h4 id="1-5-6-FeedForward"><a href="#1-5-6-FeedForward" class="headerlink" title="1.5.6 FeedForward"></a>1.5.6 FeedForward</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config: DCFormerConfig</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.w1 = nn.Linear(config.dim, config.intermediate_size, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.w3 = nn.Linear(config.dim, config.intermediate_size, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.w2 = nn.Linear(config.intermediate_size, config.dim, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.w2(F.silu(<span class="variable language_">self</span>.w1(x)) * <span class="variable language_">self</span>.w3(x))</span><br></pre></td></tr></table></figure>

<p>这部分是前馈网络模块，典型的 Transformer 组件：<br>-两个全连接层。<br>-使用 SILU 激活函数。</p>
<h4 id="1-5-7-RMSNorm"><a href="#1-5-7-RMSNorm" class="headerlink" title="1.5.7 RMSNorm"></a>1.5.7 RMSNorm</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RMSNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim: <span class="built_in">int</span>, eps: <span class="built_in">float</span> = <span class="number">1e-5</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.eps = eps</span><br><span class="line">        <span class="variable language_">self</span>.weight = nn.Parameter(torch.ones(dim))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_norm</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> x * torch.rsqrt(torch.mean(x * x, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>) + <span class="variable language_">self</span>.eps)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        output = <span class="variable language_">self</span>._norm(x.<span class="built_in">float</span>()).type_as(x)</span><br><span class="line">        <span class="keyword">return</span> output * <span class="variable language_">self</span>.weight</span><br></pre></td></tr></table></figure>

<p>RMSNorm 是一种替代 LayerNorm 的归一化技术，具有更好的训练稳定性。<br><strong>公式</strong>:<br>$$<br>\text{output} &#x3D; \frac{x}{\sqrt{\text{mean}(x^2) + \epsilon}} \cdot \text{scale}<br>$$</p>
<h4 id="1-5-8-辅助函数"><a href="#1-5-8-辅助函数" class="headerlink" title="1.5.8 辅助函数"></a>1.5.8 辅助函数</h4><p><strong>precompute_freqs_cis</strong><br>-用于预计算旋转位置编码（RoPE）。<br>-将频率参数以极坐标形式表示，支持快速计算旋转编码。</p>
<p><strong>make_window_mask</strong><br>生成窗口掩码，仅允许模型关注特定上下文范围。</p>
<p><strong>apply_rotary_emb</strong><br>应用旋转位置编码，增强 Transformer 在长序列上的建模能力。</p>
<p><strong>slice_dw</strong><br>切片动态权重，用于按窗口分割输入。</p>
<h2 id="2-DCpythia"><a href="#2-DCpythia" class="headerlink" title="2 DCpythia"></a>2 DCpythia</h2><h3 id="config-json"><a href="#config-json" class="headerlink" title="config.json"></a>config.json</h3><p>本文件定义了模型的各种配置参数，包括架构、token的id、模型的维度等。该配置文件是用来加载模型时给定的超参数和设置。</p>
<p>主要配置项：<br><strong>architectures:</strong> 定义模型的架构类型，这里指定为 “DCPythia”，即模型是基于DCPythia架构。<br><strong>auto_map:</strong> 映射自动配置类和模型类，AutoConfig指向 configuration_dcpythia.DCPythiaConfig，而 AutoModelForCausalLM 指向 modeling_dcpythia.DCPythia。<br><strong>block_size:</strong> 定义块的大小，通常影响模型处理的最大序列长度。<br><strong>vocab_size:</strong> 模型词汇表的大小，通常影响模型的输入和输出维度。<br><strong>dim:</strong> 模型的隐藏层维度，也就是每个token的表示向量的大小。<br><strong>n_layer:</strong> 模型的层数，也就是Transformer的堆叠层数。<br><strong>n_head:</strong> 每个自注意力层的头数。<br><strong>intermediate_size:</strong> Transformer中FeedForward层的维度，影响每层的宽度。<br><strong>bos_token_id, eos_token_id:</strong> 序列的起始token和结束token的ID。<br><strong>torch_dtype:</strong> 定义使用的PyTorch数据类型，这里使用float16，表示使用半精度浮点数，通常用于节省内存和加速计算。<br><strong>use_dcmha:</strong> 是否使用DCMHA（Dynamic Chunked Multi-Head Attention）。<br><strong>use_parallel_residual:</strong> 是否启用并行残差连接。<br><strong>use_linear_bias:</strong> 是否使用线性偏置。<br><strong>use_qk_norm:</strong> 是否在自注意力计算中使用查询（Q）和键（K）的归一化。<br><strong>transformers_version:</strong> 定义所使用的Transformers库版本。</p>
<h3 id="configuration-dcythia-py"><a href="#configuration-dcythia-py" class="headerlink" title="configuration_dcythia.py"></a>configuration_dcythia.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers.configuration_utils <span class="keyword">import</span> PretrainedConfig</span><br><span class="line"><span class="keyword">from</span> transformers.utils <span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Optional</span>, <span class="type">Tuple</span>, <span class="type">List</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DCPythiaConfig</span>(<span class="title class_ inherited__">PretrainedConfig</span>):</span><br><span class="line">    model_type = <span class="string">&quot;dcpythia&quot;</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    DCPythiaConfig is a config class for DCPythia, which is adapted from</span></span><br><span class="line"><span class="string">    https://github.com/pytorch-labs/gpt-fast/blob/main/model.py#L21</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>DCPythiaConfig 继承了 PretrainedConfig，这使得它能够作为 Hugging Face transformers 库中的配置类来使用。配置类用于管理模型的参数和设置。<br>model_type 定义了模型类型为 “dcpythia”，这表明该配置是为 DCPythia 模型定制的。<br>注释表明该配置类来自某个 GitHub 项目链接，表明 DCPythia 是从 GPT-fast 模型中改编的。</p>
<p><strong>初始化方法 (<strong>init</strong>)：</strong><br>这是 DCPythiaConfig 类的初始化方法，定义了很多用于 DCPythia 模型的超参数。<br>常见的超参数包括：<br>block_size：序列的最大长度，通常决定输入的最大 token 数量。<br>vocab_size：词汇表的大小。<br>n_layer：Transformer 网络的层数。<br>n_head：多头注意力机制中的头数。<br>dim：模型的隐层维度。<br>intermediate_size：中间层的维度，通常是 dim 的倍数。<br>head_dim：每个头的维度，通常 dim &#x2F; n_head。<br>use_gradient_checkpointing：是否启用梯度检查点，以节省内存。<br>use_dcmha：是否使用 DC-MHA（假设是某种改进的多头注意力机制）。<br>use_qk_norm：是否对查询-键（Q和K）做归一化。<br>window_size 和 window_type：窗口大小和类型，可能用于局部注意力机制。<br>rotary_pct：用于旋转位置编码的百分比。</p>
<p>接着是后处理部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.n_local_heads == -<span class="number">1</span>:</span><br><span class="line">    <span class="variable language_">self</span>.n_local_heads = <span class="variable language_">self</span>.n_head</span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.intermediate_size <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="variable language_">self</span>.intermediate_size = <span class="number">4</span> * <span class="variable language_">self</span>.dim</span><br><span class="line"><span class="variable language_">self</span>.head_dim = <span class="variable language_">self</span>.dim // <span class="variable language_">self</span>.n_head</span><br></pre></td></tr></table></figure>
<p>如果 n_local_heads 没有被指定（即为 -1），则将其设置为 n_head。<br>如果没有指定 intermediate_size，则将其设置为 dim 的四倍。<br>head_dim 被计算为 dim &#x2F;&#x2F; n_head。</p>
<p>然后调用父类的构造方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">super</span>().__init__(</span><br><span class="line">    pad_token_id=pad_token_id,</span><br><span class="line">    bos_token_id=bos_token_id,</span><br><span class="line">    eos_token_id=eos_token_id,</span><br><span class="line">    tie_word_embeddings=tie_word_embeddings,</span><br><span class="line">    **kwargs,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这里调用了父类 PretrainedConfig 的构造方法，并传递了额外的 token ID 和其他参数。</p>
<h3 id="generation-demo-py"><a href="#generation-demo-py" class="headerlink" title="generation_demo.py"></a>generation_demo.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;TOKENIZERS_PARALLELISM&#x27;</span>] = <span class="string">&#x27;false&#x27;</span></span><br></pre></td></tr></table></figure>
<p>导入 torch 和 transformers 库，用于加载模型和执行推理。<br>关闭 TOKENIZERS_PARALLELISM 环境变量，通常用于避免多线程令牌化过程中的问题。</p>
<p>加载模型和分词器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;Caiyun-AI/DCPythia-6.9B&quot;</span>)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(<span class="string">&quot;Caiyun-AI/DCPythia-6.9B&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>使用 AutoTokenizer 和 AutoModelForCausalLM 从指定的模型库 “Caiyun-AI&#x2F;DCPythia-6.9B” 加载模型和分词器。<br>trust_remote_code&#x3D;True 表示信任并加载远程代码，可能包含自定义实现。</p>
<p>设置设备：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>将模型和推理过程设置为在 GPU 上运行。</p>
<p>配置模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">MAX_BATCH_SIZE = <span class="number">1</span></span><br><span class="line">MAX_SEQ_LENGTH = <span class="number">2048</span></span><br><span class="line">NUM_TOKENS_TO_GENERATE = <span class="number">100</span></span><br><span class="line">COMPILE = <span class="literal">True</span></span><br><span class="line">_ = model.to(device=device, dtype=torch.float16)</span><br></pre></td></tr></table></figure>
<p>设置批量大小、最大序列长度和生成的 token 数量。<br>COMPILE &#x3D; True 表示是否启用编译功能（即是否使用 torch.compile）。<br>将模型转移到 GPU 上，并设置数据类型为 float16 以减少内存使用。</p>
<p>设置缓存：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.device(device):</span><br><span class="line">    model.setup_caches(max_batch_size=MAX_BATCH_SIZE, max_seq_length=MAX_SEQ_LENGTH, set_kv_cache=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>在指定的设备上配置模型的缓存。这里指定了批处理大小、最大序列长度，并启用了键值缓存。</p>
<p>定义 decode_one_token 函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">decode_one_token</span>(<span class="params">model, cur_token, input_pos</span>):</span><br><span class="line">    logits = model(cur_token, input_pos=input_pos, return_tensor=<span class="literal">True</span>)</span><br><span class="line">    new_token = torch.argmax(logits[:, -<span class="number">1</span>], dim=-<span class="number">1</span>)[:, <span class="literal">None</span>]</span><br><span class="line">    <span class="keyword">return</span> new_token</span><br></pre></td></tr></table></figure>
<p>定义了一个函数 decode_one_token，它根据当前 token 和位置生成下一个 token。模型输出的 logits 被用来选择概率最大的 token。</p>
<p>生成文本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">prompt = <span class="string">&quot;Beijing is the capital of China. London is the capital of&quot;</span></span><br><span class="line">input_ids = tokenizer.encode(prompt, return_tensors=<span class="string">&#x27;pt&#x27;</span>)</span><br><span class="line">compiled_decode_one_token = torch.<span class="built_in">compile</span>(decode_one_token, mode=<span class="string">&quot;reduce-overhead&quot;</span>, fullgraph=<span class="literal">True</span>) <span class="keyword">if</span> COMPILE <span class="keyword">else</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p>使用给定的 prompt 编码为 token ids。<br>torch.compile 对 decode_one_token 函数进行优化，减少计算开销。</p>
<p>推理生成：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    generated_ids = model.generate(input_ids.to(device), num_tokens_to_generate=NUM_TOKENS_TO_GENERATE, compiled_decode_one_token=compiled_decode_one_token)</span><br><span class="line">    text = tokenizer.decode(generated_ids[<span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;generated text:&#x27;</span>, text)</span><br></pre></td></tr></table></figure>
<p>使用 model.generate 方法生成文本。<br>使用 tokenizer.decode 将生成的 token 转换为文本并打印出来。</p>
<h3 id="modeling-dcpythia-py"><a href="#modeling-dcpythia-py" class="headerlink" title="modeling_dcpythia.py"></a>modeling_dcpythia.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">KVKWCache</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, max_batch_size, max_seq_length, n_heads, head_dim, window_size=<span class="number">2048</span>, dtype=torch.float16, use_kw_cache=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.head_dim = head_dim</span><br><span class="line">        <span class="variable language_">self</span>.kw_dim = <span class="number">2</span> * n_heads </span><br><span class="line">        <span class="variable language_">self</span>.n_heads = n_heads</span><br><span class="line">        <span class="variable language_">self</span>.window_size = window_size</span><br><span class="line">        <span class="variable language_">self</span>.use_kw_cache = use_kw_cache </span><br><span class="line">        <span class="keyword">if</span> window_size <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="variable language_">self</span>.seq_length = max_seq_length</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.seq_length = <span class="built_in">min</span>(window_size, max_seq_length)</span><br><span class="line">        cache_shape = (max_batch_size, n_heads, <span class="variable language_">self</span>.seq_length, head_dim)</span><br><span class="line">        kw_cache_shape = (max_batch_size, <span class="variable language_">self</span>.seq_length, <span class="number">2</span>, n_heads, n_heads)</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;k_cache&#x27;</span>, torch.zeros(cache_shape, dtype=dtype))</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;v_cache&#x27;</span>, torch.zeros(cache_shape, dtype=dtype))</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_kw_cache:</span><br><span class="line">            <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;kw_cache&#x27;</span>, torch.zeros(kw_cache_shape, dtype=dtype))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, input_pos, k_val, v_val, kw_val=<span class="literal">None</span></span>): <span class="comment"># kw_val B,N,S,2,N      B2NSD</span></span><br><span class="line">        <span class="comment"># input_pos: [S], k_val: [B, H, S, D]</span></span><br><span class="line">        <span class="keyword">assert</span> input_pos.shape[-<span class="number">1</span>] == k_val.shape[<span class="number">2</span>]</span><br><span class="line">        B,N,S,D = v_val.shape</span><br><span class="line">        k_out = <span class="variable language_">self</span>.k_cache</span><br><span class="line">        v_out = <span class="variable language_">self</span>.v_cache</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_kw_cache:</span><br><span class="line">            kw_out = <span class="variable language_">self</span>.kw_cache</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            kw_out = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.window_size <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            k_out[:, :, input_pos] = k_val</span><br><span class="line">            v_out[:, :, input_pos] = v_val</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.use_kw_cache <span class="keyword">and</span> kw_val <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                kw_out[:,input_pos] = kw_val</span><br><span class="line">        <span class="keyword">elif</span> S == <span class="number">1</span>: </span><br><span class="line">            input_pos = input_pos % <span class="variable language_">self</span>.seq_length</span><br><span class="line">            v_out[:, :, input_pos] = v_val</span><br><span class="line">            k_out[:, :, input_pos] = k_val</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.use_kw_cache <span class="keyword">and</span> kw_val <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                kw_out[:,input_pos] = kw_val</span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># prefill</span></span><br><span class="line">            start = <span class="built_in">max</span>(<span class="number">0</span>, input_pos[-<span class="number">1</span>]-<span class="variable language_">self</span>.seq_length+<span class="number">1</span>)</span><br><span class="line">            input_pos = input_pos[start:] % <span class="variable language_">self</span>.seq_length</span><br><span class="line">            v_out[:, :, input_pos] = v_val[:,:,start:]</span><br><span class="line">            k_out[:, :, input_pos] = k_val[:,:,start:]</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.use_kw_cache <span class="keyword">and</span> kw_val <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                kw_out[:, input_pos] = kw_val[:,start:]</span><br><span class="line">        <span class="keyword">return</span> k_out, v_out, kw_out </span><br></pre></td></tr></table></figure>
<p>用于管理模型的键值缓存，适配不同的序列长度、窗口大小和头部配置。<br>支持常规键值缓存 (k_cache 和 v_cache) 和额外的 kw_cache。<br>方法 update 用于更新缓存，是模型的核心之一，处理序列窗口截断和序列填充。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DCPythia</span>(<span class="title class_ inherited__">PreTrainedModel</span>):</span><br><span class="line">    config_class=DCPythiaConfig</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config: DCPythiaConfig</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__(config)</span><br><span class="line">        <span class="variable language_">self</span>.config = config</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.tok_embeddings = nn.Embedding(config.vocab_size, config.dim)</span><br><span class="line">        <span class="variable language_">self</span>.layers = nn.ModuleList(DCPythiaBlock(config, lidx) <span class="keyword">for</span> lidx <span class="keyword">in</span> <span class="built_in">range</span>(config.n_layer))</span><br><span class="line">        <span class="variable language_">self</span>.norm = nn.LayerNorm(config.dim, eps=config.norm_eps)</span><br><span class="line">        <span class="variable language_">self</span>.output = nn.Linear(config.dim, config.vocab_size, bias=<span class="literal">False</span>) <span class="comment"># no bias in pythia</span></span><br><span class="line">        <span class="variable language_">self</span>.use_gradient_checkpointing = config.use_gradient_checkpointing </span><br><span class="line">        <span class="variable language_">self</span>.is_training = config.is_training</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.freqs_cis: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.rotary_ndims = <span class="built_in">int</span>(config.head_dim * config.rotary_pct)</span><br><span class="line">        <span class="variable language_">self</span>.mask_cache: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.window_size = config.window_size</span><br><span class="line">        <span class="variable language_">self</span>.max_batch_size = -<span class="number">1</span></span><br><span class="line">        <span class="variable language_">self</span>.max_seq_length = -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">setup_caches</span>(<span class="params">self, max_batch_size, max_seq_length, set_kv_cache=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.max_seq_length &gt;= max_seq_length <span class="keyword">and</span> <span class="variable language_">self</span>.max_batch_size &gt;= max_batch_size:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        head_dim = <span class="variable language_">self</span>.config.dim // <span class="variable language_">self</span>.config.n_head</span><br><span class="line">        max_seq_length = find_multiple(max_seq_length, <span class="number">8</span>)</span><br><span class="line">        <span class="variable language_">self</span>.max_seq_length = max_seq_length</span><br><span class="line">        <span class="variable language_">self</span>.max_batch_size = max_batch_size</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.is_training:</span><br><span class="line">            <span class="keyword">for</span> b <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line">                <span class="keyword">if</span> set_kv_cache:</span><br><span class="line">                    use_kw_cache = <span class="literal">False</span> <span class="keyword">if</span> b.attention.query_wise <span class="keyword">else</span> <span class="literal">True</span></span><br><span class="line">                    b.attention.kv_cache = KVKWCache(max_batch_size, max_seq_length, <span class="variable language_">self</span>.config.n_local_heads, head_dim, window_size=b.attention.window_size, use_kw_cache=use_kw_cache)</span><br><span class="line">                b.attention.dyn_w_proj.merge_weights()</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> b.attention.use_sw:</span><br><span class="line">                    dtype = b.attention.wo.weight.dtype</span><br><span class="line">                    device = b.attention.wo.weight.device</span><br><span class="line">                    b.attention.dyn_w_proj.sw = b.attention.dyn_w_proj.sw.to(device=device, dtype=dtype)</span><br><span class="line">                    b.attention.dyn_w_proj.pre_proj.w = b.attention.dyn_w_proj.pre_proj.w.to(device=device, dtype=dtype) </span><br><span class="line">                    b.attention.dyn_w_proj.post_proj.w = b.attention.dyn_w_proj.post_proj.w.to(device=device, dtype=dtype) </span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.freqs_cis = precompute_freqs_cis(<span class="variable language_">self</span>.config.block_size, <span class="variable language_">self</span>.rotary_ndims, <span class="variable language_">self</span>.config.rope_base).to(<span class="variable language_">self</span>.tok_embeddings.weight.device)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.is_training:</span><br><span class="line">            <span class="variable language_">self</span>.causal_mask = torch.tril(torch.ones(<span class="variable language_">self</span>.config.block_size, <span class="variable language_">self</span>.config.block_size, dtype=torch.<span class="built_in">bool</span>, device=<span class="variable language_">self</span>.tok_embeddings.weight.device))</span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.window_size <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="variable language_">self</span>.causal_mask = torch.tril(torch.ones(max_seq_length, max_seq_length, dtype=torch.<span class="built_in">bool</span>, device=<span class="variable language_">self</span>.tok_embeddings.weight.device))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.causal_mask = torch.stack([make_window_mask(max_seq_length, <span class="variable language_">self</span>.config.window_size), torch.tril(torch.ones(<span class="variable language_">self</span>.max_seq_length, <span class="variable language_">self</span>.max_seq_length, dtype=torch.<span class="built_in">bool</span>))]) <span class="comment"># LG</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">self, input_ids, num_tokens_to_generate=<span class="number">10</span>, compiled_decode_one_token=<span class="literal">None</span></span>):</span><br><span class="line">        batch_size, seq_length = input_ids.shape</span><br><span class="line">        input_pos = torch.arange(seq_length, device=<span class="variable language_">self</span>.device)</span><br><span class="line">        generated_ids = torch.zeros(batch_size, seq_length + num_tokens_to_generate, dtype=torch.<span class="built_in">int</span>, device=<span class="variable language_">self</span>.device)</span><br><span class="line">        generated_ids[:, :seq_length] = input_ids.to(<span class="variable language_">self</span>.device).to(torch.<span class="built_in">int</span>)</span><br><span class="line">        logits = <span class="variable language_">self</span>.forward(input_ids, input_pos=input_pos,return_tensor=<span class="literal">True</span>)</span><br><span class="line">        _next_token = torch.argmax(logits[:, -<span class="number">1</span>], dim=-<span class="number">1</span>)[:, <span class="literal">None</span>]</span><br><span class="line">        next_token = torch.zeros(<span class="variable language_">self</span>.max_batch_size, <span class="number">1</span>, device=<span class="variable language_">self</span>.device, dtype=torch.<span class="built_in">int</span>)</span><br><span class="line">        next_token[:batch_size] = _next_token</span><br><span class="line">        generated_ids[:, seq_length] = next_token[:batch_size, <span class="number">0</span>]</span><br><span class="line">        input_pos = torch.tensor([seq_length], device=<span class="variable language_">self</span>.device)</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num_tokens_to_generate):</span><br><span class="line">            <span class="keyword">if</span> compiled_decode_one_token <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                next_token = compiled_decode_one_token(<span class="variable language_">self</span>, next_token.clone(), input_pos)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                next_token = <span class="variable language_">self</span>.decode_one_token(next_token.clone(), input_pos)</span><br><span class="line">            generated_ids[:, input_pos+<span class="number">1</span>] = next_token.<span class="built_in">int</span>()[:batch_size]</span><br><span class="line">            input_pos += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> generated_ids</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode_one_token</span>(<span class="params">self, cur_token, input_pos</span>):</span><br><span class="line">        logits = <span class="variable language_">self</span>.forward(</span><br><span class="line">            cur_token,</span><br><span class="line">            input_pos=input_pos,</span><br><span class="line">            return_tensor=<span class="literal">True</span>,</span><br><span class="line">        )</span><br><span class="line">        new_token = torch.argmax(logits[:, -<span class="number">1</span>], dim=-<span class="number">1</span>)[:,<span class="literal">None</span>]</span><br><span class="line">        <span class="keyword">return</span> new_token</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, idx: Tensor, input_pos: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, return_tensor=<span class="literal">False</span></span>) -&gt; Tensor:</span><br><span class="line">        <span class="keyword">assert</span> <span class="variable language_">self</span>.freqs_cis <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>, <span class="string">&quot;Caches must be initialized first&quot;</span></span><br><span class="line">        <span class="keyword">if</span> input_pos <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            input_pos = torch.arange(idx.shape[-<span class="number">1</span>], device=idx.device, dtype=torch.<span class="built_in">int</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.window_size <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> <span class="variable language_">self</span>.is_training:</span><br><span class="line">            mask = <span class="variable language_">self</span>.causal_mask[<span class="literal">None</span>, <span class="literal">None</span>, input_pos]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mask = <span class="variable language_">self</span>.causal_mask[<span class="literal">None</span>, <span class="literal">None</span>,:,input_pos]</span><br><span class="line">        freqs_cis = <span class="variable language_">self</span>.freqs_cis[input_pos][:idx.shape[-<span class="number">1</span>]]</span><br><span class="line">        x = <span class="variable language_">self</span>.tok_embeddings(idx)</span><br><span class="line">        <span class="keyword">for</span> i, layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.layers):</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.is_training <span class="keyword">or</span> <span class="variable language_">self</span>.window_size <span class="keyword">is</span> <span class="literal">None</span> :</span><br><span class="line">                layer_mask = mask</span><br><span class="line">                gen_mask = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">elif</span> <span class="variable language_">self</span>.window_size <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: </span><br><span class="line">                layer_mask = mask[:,:,<span class="number">1</span>] <span class="keyword">if</span> layer.attention.window_size <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> mask[:,:,<span class="number">0</span>]</span><br><span class="line">                gen_mask = mask[:,:,<span class="number">1</span>] <span class="keyword">if</span> layer.attention.window_size <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.use_gradient_checkpointing:</span><br><span class="line">                x = checkpoint(layer, x, input_pos, freqs_cis, layer_mask)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                x = layer(x, input_pos, freqs_cis, layer_mask, gen_mask=gen_mask)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm(x)</span><br><span class="line">        logits = <span class="variable language_">self</span>.output(x)</span><br><span class="line">        <span class="keyword">if</span> return_tensor:</span><br><span class="line">            <span class="keyword">return</span> logits</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            CausalLMOutput = namedtuple(<span class="string">&quot;CausalLMOutput&quot;</span>, [<span class="string">&quot;logits&quot;</span>])</span><br><span class="line">            <span class="keyword">return</span> CausalLMOutput(logits=logits)</span><br></pre></td></tr></table></figure>
<p>基于 HuggingFace 的 PreTrainedModel，是该模型的主干结构。<br>包括以下子模块：<br>-tok_embeddings：词嵌入层。<br>-多层 DCPythiaBlock。<br>-LayerNorm 和输出线性层。<br>-支持训练和推理两种模式，拥有灵活的缓存管理功能。<br>方法 setup_caches 初始化模型缓存，确保性能优化。<br>方法 generate 实现逐步生成，支持动态调整输入序列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DCPythiaBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config: DCPythiaConfig, lidx</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.lidx = lidx</span><br><span class="line">        <span class="variable language_">self</span>.attention = DCMHAttention(config, lidx)</span><br><span class="line">        <span class="variable language_">self</span>.feed_forward = FeedForward(config)</span><br><span class="line">        <span class="variable language_">self</span>.ffn_norm = nn.LayerNorm(config.dim, eps=config.norm_eps)</span><br><span class="line">        <span class="variable language_">self</span>.attention_norm = nn.LayerNorm(config.dim, eps=config.norm_eps)</span><br><span class="line">        <span class="variable language_">self</span>.use_parallel_residual = config.use_parallel_residual</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor, input_pos: Tensor, freqs_cis: Tensor, mask: Tensor, gen_mask=<span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">        h = x + <span class="variable language_">self</span>.attention(<span class="variable language_">self</span>.attention_norm(x), freqs_cis, mask, input_pos, fast_infer=<span class="literal">True</span>, gen_mask=gen_mask)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_parallel_residual:</span><br><span class="line">            out = h + <span class="variable language_">self</span>.feed_forward(<span class="variable language_">self</span>.ffn_norm(x))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            out = h + <span class="variable language_">self</span>.feed_forward(<span class="variable language_">self</span>.ffn_norm(h))</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>表示模型的一层，包含注意力机制和前馈网络。<br>使用并行残差结构（use_parallel_residual）来优化计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DynamicWeightProjection</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_heads=<span class="number">32</span>, num_groups=<span class="number">1</span>, residual=<span class="literal">True</span>, query_input_dim=<span class="number">4096</span>, dynamic_squeeze_ratio=<span class="number">16</span>, dynamic_w_hidden_dim=<span class="number">128</span>,dtype=torch.float16,use_sw=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads </span><br><span class="line">        <span class="variable language_">self</span>.num_groups = num_groups </span><br><span class="line">        <span class="variable language_">self</span>.query_input_dim = query_input_dim </span><br><span class="line">        <span class="variable language_">self</span>.dynamic_squeeze_ratio = dynamic_squeeze_ratio</span><br><span class="line">        <span class="variable language_">self</span>.dynamic_w_hidden_dim = dynamic_w_hidden_dim </span><br><span class="line">        <span class="variable language_">self</span>.dw_hidden_activation = nn.GELU()</span><br><span class="line">        <span class="variable language_">self</span>.num_heads_per_group = <span class="variable language_">self</span>.num_heads // <span class="variable language_">self</span>.num_groups</span><br><span class="line">        <span class="variable language_">self</span>.dw_activation = nn.Tanh()</span><br><span class="line">        <span class="variable language_">self</span>.dw1_norm = RMSnormNoscale(dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.use_sw = use_sw</span><br><span class="line">        <span class="variable language_">self</span>.pre_proj = CrossHeadProjection(<span class="string">&#x27;pre&#x27;</span>, num_heads=<span class="variable language_">self</span>.num_heads, use_sw=use_sw)</span><br><span class="line">        <span class="variable language_">self</span>.post_proj = CrossHeadProjection(<span class="string">&#x27;post&#x27;</span>, num_heads=<span class="variable language_">self</span>.num_heads, use_sw=use_sw)</span><br><span class="line"></span><br><span class="line">        dynamic_hidden_dim = <span class="variable language_">self</span>.num_heads_per_group // <span class="variable language_">self</span>.dynamic_squeeze_ratio </span><br><span class="line">        <span class="variable language_">self</span>.dynamic_hidden_dim = dynamic_hidden_dim </span><br><span class="line">        <span class="variable language_">self</span>.dw1 = nn.parameter.Parameter(torch.zeros(<span class="variable language_">self</span>.query_input_dim, <span class="variable language_">self</span>.num_groups, <span class="number">4</span>, <span class="variable language_">self</span>.dynamic_w_hidden_dim, dtype=dtype)) <span class="comment">#(4096, 1, 4, 128)</span></span><br><span class="line">        G, K, M = <span class="variable language_">self</span>.num_groups, <span class="variable language_">self</span>.dynamic_w_hidden_dim, <span class="variable language_">self</span>.num_heads_per_group</span><br><span class="line">        I = dynamic_hidden_dim * <span class="number">2</span> </span><br><span class="line">        <span class="variable language_">self</span>.qkw = nn.parameter.Parameter(torch.zeros([G, <span class="number">4</span>, K, I, M], dtype=dtype)) <span class="comment"># (1, 4, 128, 4, 32)</span></span><br><span class="line">        <span class="variable language_">self</span>.dd = nn.parameter.Parameter(torch.zeros(<span class="variable language_">self</span>.query_input_dim, <span class="variable language_">self</span>.num_groups, <span class="variable language_">self</span>.num_heads_per_group * <span class="number">4</span>, dtype=dtype)) <span class="comment">#  (4096, 1, 128)</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.merge_weights()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">merge_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.dw_m = nn.parameter.Parameter(torch.cat([<span class="variable language_">self</span>.dw1.reshape(<span class="variable language_">self</span>.query_input_dim, -<span class="number">1</span>), <span class="variable language_">self</span>.dd.squeeze(<span class="number">1</span>)], dim=-<span class="number">1</span>)).to(<span class="variable language_">self</span>.dw1.device) <span class="comment"># E,(4*K + K)  K=2*N*I</span></span><br><span class="line">        <span class="variable language_">self</span>.qkw_m = nn.parameter.Parameter(<span class="variable language_">self</span>.qkw.permute(<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>).reshape(<span class="number">4</span>,<span class="variable language_">self</span>.dynamic_w_hidden_dim,-<span class="number">1</span>)).to(<span class="variable language_">self</span>.dw1.device) <span class="comment">#(4,K,I*M)</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_sw:</span><br><span class="line">            <span class="variable language_">self</span>.sw = nn.parameter.Parameter(torch.stack([<span class="variable language_">self</span>.pre_proj.w, <span class="variable language_">self</span>.post_proj.w]).squeeze(<span class="number">1</span>) + torch.eye(<span class="variable language_">self</span>.num_heads) ).to(<span class="variable language_">self</span>.dw1.device) <span class="comment"># (2,N,N) sw + identity matrix</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.sw = (torch.eye(<span class="variable language_">self</span>.num_heads).expand(<span class="number">2</span>,<span class="variable language_">self</span>.num_heads,<span class="variable language_">self</span>.num_heads)).to(<span class="variable language_">self</span>.dw1.device) <span class="comment"># identity matrix (2,N,N)</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,query_vec,KW:<span class="type">Optional</span>[torch.Tensor]=<span class="literal">None</span>, gen_cache:<span class="type">Optional</span>[<span class="built_in">bool</span>]=<span class="literal">True</span></span>):  </span><br><span class="line">        dw_hidden = torch.einsum(<span class="string">&#x27;BTD,DGCK-&gt;BTGCK&#x27;</span>, query_vec, <span class="variable language_">self</span>.dw1)  <span class="comment"># C=4 [pre,post]*[query,key]</span></span><br><span class="line">        dw_hidden = <span class="variable language_">self</span>.dw_hidden_activation(dw_hidden) <span class="comment">#BTGCK</span></span><br><span class="line">        w1, w2 = torch.split(torch.einsum(<span class="string">&#x27;BTGCK,GCKIM-&gt;BTGCIM&#x27;</span>, dw_hidden, <span class="variable language_">self</span>.qkw), <span class="variable language_">self</span>.qkw.shape[-<span class="number">2</span>]//<span class="number">2</span>, dim=-<span class="number">2</span>) <span class="comment">#BTGC(2I)M -&gt; [BTGCIM] * 2</span></span><br><span class="line">        w1 = <span class="variable language_">self</span>.dw1_norm(w1) <span class="comment"># BTGCIM</span></span><br><span class="line">        pre_qw1, pre_kw1, post_qw1, post_kw1 = unbind(w1, <span class="number">4</span>, dim=<span class="number">3</span>) <span class="comment"># BTG4IM-&gt;[BTGIM]*4</span></span><br><span class="line">        pre_qw2, pre_kw2, post_qw2, post_kw2 = unbind(w2, <span class="number">4</span>, dim=<span class="number">3</span>) </span><br><span class="line">        dd = torch.einsum(<span class="string">&#x27;BTD,DGM-&gt;BTGM&#x27;</span>, query_vec, <span class="variable language_">self</span>.dd) <span class="comment"># BTG(4M)</span></span><br><span class="line">        dd = <span class="variable language_">self</span>.dw_activation(dd)</span><br><span class="line">        pre_qdd, pre_kdd, post_qdd, post_kdd = torch.split(dd, dd.shape[-<span class="number">1</span>] // <span class="number">4</span>, dim=-<span class="number">1</span>) <span class="comment"># BTG(4N)-&gt;[BTGN]*4</span></span><br><span class="line">        pre_dw_args = (pre_qw1, pre_qw2, pre_kw1, pre_kw2, pre_qdd, pre_kdd)</span><br><span class="line">        post_dw_args = (post_qw1, post_qw2, post_kw1, post_kw2, post_qdd, post_kdd)</span><br><span class="line">        <span class="keyword">if</span> gen_cache: <span class="comment"># generate KW cache</span></span><br><span class="line">            pre_kw = torch.einsum(<span class="string">&#x27;BSGIM, BSGIN-&gt;BSMN&#x27;</span>, pre_kw1, pre_kw2) + torch.diag_embed(pre_kdd.squeeze(<span class="number">2</span>))  <span class="comment"># merge kw and kdd</span></span><br><span class="line">            post_kw = torch.einsum(<span class="string">&#x27;BSGIM, BSGIN-&gt;BSMN&#x27;</span>, post_kw1, post_kw2) + torch.diag_embed(post_kdd.squeeze(<span class="number">2</span>))</span><br><span class="line">            KW = torch.stack((pre_kw, post_kw), dim=-<span class="number">3</span>) <span class="comment"># BSMN,BSMN-&gt;BS2MN</span></span><br><span class="line">        <span class="keyword">return</span> pre_dw_args, post_dw_args, KW</span><br></pre></td></tr></table></figure>
<p>一个动态权重投影模块，用于生成自适应的权重矩阵。<br>通过输入序列特征动态调整注意力计算的权重。<br>提供高效的权重生成和缓存机制。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DCMHAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config: DCPythiaConfig, lidx, use_sw=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">assert</span> config.dim % config.n_head == <span class="number">0</span></span><br><span class="line">        total_head_dim = (config.n_head + <span class="number">2</span> * config.n_local_heads) * config.head_dim</span><br><span class="line">        <span class="comment"># key, query, value projections for all heads, but in a batch</span></span><br><span class="line">        <span class="variable language_">self</span>.lidx = lidx</span><br><span class="line">        <span class="variable language_">self</span>.wqkv = nn.Linear(config.dim, total_head_dim, bias=config.use_linear_bias)</span><br><span class="line">        <span class="variable language_">self</span>.wo = nn.Linear(config.dim, config.dim, bias=config.use_linear_bias)</span><br><span class="line">        <span class="variable language_">self</span>.kv_cache = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.n_head = config.n_head</span><br><span class="line">        <span class="variable language_">self</span>.head_dim = config.head_dim</span><br><span class="line">        <span class="variable language_">self</span>.n_local_heads = config.n_local_heads</span><br><span class="line">        <span class="variable language_">self</span>.is_training = config.is_training</span><br><span class="line">        <span class="variable language_">self</span>.dim = config.dim</span><br><span class="line">        <span class="variable language_">self</span>.use_dcmha = config.use_dcmha </span><br><span class="line">        <span class="variable language_">self</span>.scale_factor = <span class="number">1</span> / math.sqrt(<span class="variable language_">self</span>.head_dim)</span><br><span class="line">        <span class="variable language_">self</span>.q_chunk_size = config.q_chunk_size </span><br><span class="line">        <span class="variable language_">self</span>.use_sw = use_sw </span><br><span class="line">        <span class="variable language_">self</span>.dyn_w_proj = DynamicWeightProjection(num_heads=<span class="variable language_">self</span>.n_head, query_input_dim=config.dim, dynamic_squeeze_ratio=<span class="variable language_">self</span>.n_head//<span class="number">2</span>, dynamic_w_hidden_dim=<span class="variable language_">self</span>.n_head*<span class="number">4</span>, use_sw=use_sw)</span><br><span class="line">        <span class="variable language_">self</span>.use_qk_norm = config.use_qk_norm </span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_qk_norm:</span><br><span class="line">            <span class="variable language_">self</span>.q_norm = RMSnorm(hid_dim=<span class="variable language_">self</span>.head_dim)</span><br><span class="line">            <span class="variable language_">self</span>.k_norm = RMSnorm(hid_dim=<span class="variable language_">self</span>.head_dim)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.window_types = &#123;</span><br><span class="line">            <span class="string">&quot;LG&quot;</span>:[<span class="number">256</span>, <span class="literal">None</span>],</span><br><span class="line">            <span class="string">&quot;LGLL&quot;</span>:[<span class="number">256</span>, <span class="literal">None</span>, <span class="number">256</span>, <span class="number">256</span>],</span><br><span class="line">            <span class="string">&quot;LGL6&quot;</span>:[<span class="number">256</span>, <span class="literal">None</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>],</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.query_wise = config.query_wise</span><br><span class="line">        <span class="keyword">if</span> config.window_type <span class="keyword">is</span> <span class="literal">None</span>: <span class="comment"># LG </span></span><br><span class="line">            <span class="variable language_">self</span>.window_size = <span class="literal">None</span> <span class="keyword">if</span> <span class="variable language_">self</span>.lidx % <span class="number">2</span> == <span class="number">1</span> <span class="keyword">else</span> config.window_size </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            window_l = <span class="variable language_">self</span>.window_types[config.window_type]</span><br><span class="line">            <span class="variable language_">self</span>.window_size = window_l[<span class="variable language_">self</span>.lidx % <span class="built_in">len</span>(window_l)]</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.rotary_ndims = <span class="built_in">int</span>(<span class="variable language_">self</span>.head_dim * config.rotary_pct)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.is_training:</span><br><span class="line">            <span class="variable language_">self</span>._register_load_state_dict_pre_hook(<span class="variable language_">self</span>.load_hook)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load_hook</span>(<span class="params">self, state_dict, prefix, *args</span>):</span><br><span class="line">        <span class="keyword">if</span> prefix + <span class="string">&quot;wq.weight&quot;</span> <span class="keyword">in</span> state_dict:</span><br><span class="line">            wq = state_dict.pop(prefix + <span class="string">&quot;wq.weight&quot;</span>)</span><br><span class="line">            wk = state_dict.pop(prefix + <span class="string">&quot;wk.weight&quot;</span>)</span><br><span class="line">            wv = state_dict.pop(prefix + <span class="string">&quot;wv.weight&quot;</span>)</span><br><span class="line">            state_dict[prefix + <span class="string">&quot;wqkv.weight&quot;</span>] = torch.cat([wq, wk, wv])</span><br><span class="line">        <span class="keyword">if</span> prefix + <span class="string">&quot;wq.bias&quot;</span> <span class="keyword">in</span> state_dict:</span><br><span class="line">            wq_b = state_dict.pop(prefix + <span class="string">&quot;wq.bias&quot;</span>)</span><br><span class="line">            wk_b = state_dict.pop(prefix + <span class="string">&quot;wk.bias&quot;</span>)</span><br><span class="line">            wv_b = state_dict.pop(prefix + <span class="string">&quot;wv.bias&quot;</span>)</span><br><span class="line">            state_dict[prefix + <span class="string">&quot;wqkv.bias&quot;</span>] = torch.cat([wq_b, wk_b, wv_b])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_generate_fast</span>(<span class="params">self, x, input_pos, q, k, v, k_mask</span>):</span><br><span class="line">        B,T,D = x.shape</span><br><span class="line">        N,I = <span class="variable language_">self</span>.n_head, <span class="variable language_">self</span>.dyn_w_proj.dynamic_hidden_dim <span class="comment"># 32, 2</span></span><br><span class="line">        dw_hidden, dd = (x @ <span class="variable language_">self</span>.dyn_w_proj.dw_m).split([<span class="number">2</span>*<span class="number">2</span>*N*(<span class="number">2</span>*I), <span class="number">2</span>*<span class="number">2</span>*N*<span class="number">1</span>], -<span class="number">1</span>) <span class="comment"># BTD, D(4K+4N) -&gt; BT(4K+4N) -&gt; BT(4K), BT(4N)</span></span><br><span class="line">        dw_hidden = dw_hidden.view((B,T,<span class="number">4</span>,-<span class="number">1</span>,<span class="number">1</span>)) <span class="comment"># BT(4K) -&gt; BT4K1</span></span><br><span class="line">        dw = (<span class="variable language_">self</span>.dyn_w_proj.dw_hidden_activation(dw_hidden) * <span class="variable language_">self</span>.dyn_w_proj.qkw_m).<span class="built_in">sum</span>(-<span class="number">2</span>) <span class="comment"># gelu, BT4K1, 4K(IM)-&gt;BT4K(IM)-&gt;BT4(IM)</span></span><br><span class="line">        w1, w2 = dw.view((B,T,<span class="number">2</span>,<span class="number">2</span>,-<span class="number">1</span>,N)).split(I,-<span class="number">2</span>) <span class="comment"># BT4(IM)-&gt;BT&#123;pre/post&#125;&#123;q/k&#125;IM-&gt;[BT22IM] * 2</span></span><br><span class="line">        w1 = <span class="variable language_">self</span>.dyn_w_proj.dw1_norm(w1) <span class="comment"># BT22IN</span></span><br><span class="line">        qkdd = <span class="variable language_">self</span>.dyn_w_proj.dw_activation(dd.view((B,T,<span class="number">2</span>,<span class="number">2</span>,N))) <span class="comment"># BT2&#123;2&#125;N1-&gt;BT2&#123;2&#125;N tanh</span></span><br><span class="line">        qkw = torch.einsum(<span class="string">&#x27;BTKJIN,BTKJIM-&gt;BTKJNM&#x27;</span>, w1, w2) + torch.diag_embed(qkdd) <span class="comment"># j=k=2, BT2&#123;2&#125;NM q/k, pre/post</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.query_wise: <span class="comment"># <span class="doctag">TODO:</span> do not generate kw and kdd</span></span><br><span class="line">            qw, _ = qkw.unbind(<span class="number">3</span>) <span class="comment"># BS2NM</span></span><br><span class="line">            kw_new = <span class="literal">None</span></span><br><span class="line">            qw = qw + <span class="variable language_">self</span>.dyn_w_proj.sw </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            qw, kw_new = qkw.unbind(<span class="number">3</span>) <span class="comment"># BS&#123;pre/post&#125;&#123;q/k&#125;NM -&gt; BS&#123;pre/post&#125;NM * 2</span></span><br><span class="line">            kw_new = kw_new + <span class="variable language_">self</span>.dyn_w_proj.sw  <span class="comment"># BS2NM + 2NM-&gt; BS2NM </span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.kv_cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            k, v, kw_out = <span class="variable language_">self</span>.kv_cache.update(input_pos, k, v, kw_val=kw_new) <span class="comment">#BNT2M</span></span><br><span class="line">        logits = q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>) * <span class="variable language_">self</span>.scale_factor </span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.query_wise:</span><br><span class="line">            w = qw  <span class="comment"># B12NM</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            w = qw + kw_out <span class="comment"># B12NM,BS2NM -&gt; BS2NM </span></span><br><span class="line">        wl, w = w.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>).unbind(<span class="number">1</span>)  <span class="comment"># BS2NM-&gt;B2NMS-&gt;[BNMS]*2 </span></span><br><span class="line">        logits = (logits * wl).<span class="built_in">sum</span>(<span class="number">1</span>).unsqueeze(<span class="number">2</span>) <span class="comment"># BN1S, BNMS -&gt; BNMS-&gt; BMS-&gt; BM1S </span></span><br><span class="line">        min_value = torch.finfo(torch.float16).<span class="built_in">min</span></span><br><span class="line">        logits = torch.where(k_mask, logits, min_value)</span><br><span class="line">        probs = logits.softmax(-<span class="number">1</span>)</span><br><span class="line">        probs = (probs * w).<span class="built_in">sum</span>(<span class="number">1</span>).unsqueeze(<span class="number">2</span>)</span><br><span class="line">        y = probs @ v</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor, freqs_cis: Tensor, mask: Tensor, input_pos: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, fast_infer=<span class="literal">True</span>, gen_mask=<span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">        bsz, seqlen, _ = x.shape</span><br><span class="line"></span><br><span class="line">        kv_size = <span class="variable language_">self</span>.n_local_heads * <span class="variable language_">self</span>.head_dim</span><br><span class="line">        q, k, v = <span class="variable language_">self</span>.wqkv(x).split([<span class="variable language_">self</span>.dim, kv_size, kv_size], dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        q = q.view(bsz, seqlen, <span class="variable language_">self</span>.n_head, <span class="variable language_">self</span>.head_dim) <span class="comment"># BSND</span></span><br><span class="line">        k = k.view(bsz, seqlen, <span class="variable language_">self</span>.n_local_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line">        v = v.view(bsz, seqlen, <span class="variable language_">self</span>.n_local_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_qk_norm:</span><br><span class="line">            q, k = <span class="variable language_">self</span>.q_norm(q), <span class="variable language_">self</span>.k_norm(k)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.rotary_ndims == <span class="variable language_">self</span>.head_dim:</span><br><span class="line">            q = apply_rotary_emb(q, freqs_cis) <span class="comment">#BTND</span></span><br><span class="line">            k = apply_rotary_emb(k, freqs_cis)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            q_rot = q[..., : <span class="variable language_">self</span>.rotary_ndims]</span><br><span class="line">            q_pass = q[..., <span class="variable language_">self</span>.rotary_ndims :]</span><br><span class="line">            k_rot = k[..., : <span class="variable language_">self</span>.rotary_ndims]</span><br><span class="line">            k_pass = k[..., <span class="variable language_">self</span>.rotary_ndims :]</span><br><span class="line">            q_rot = apply_rotary_emb(q_rot, freqs_cis, mode=<span class="string">&#x27;half&#x27;</span>) <span class="comment">#BTND</span></span><br><span class="line">            k_rot = apply_rotary_emb(k_rot, freqs_cis, mode=<span class="string">&#x27;half&#x27;</span>)</span><br><span class="line">            q = torch.cat((q_rot, q_pass), dim=-<span class="number">1</span>)</span><br><span class="line">            k = torch.cat((k_rot, k_pass), dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        q, k, v = <span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.transpose(<span class="number">1</span>, <span class="number">2</span>), (q, k, v)) <span class="comment"># BNSD</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.is_training:</span><br><span class="line">            N, D, I = <span class="variable language_">self</span>.n_head, <span class="variable language_">self</span>.head_dim, <span class="variable language_">self</span>.dyn_w_proj.dynamic_hidden_dim; <span class="comment"># 6.7B</span></span><br><span class="line">            B,T,E = x.shape</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.use_dcmha:</span><br><span class="line">                project_logits = <span class="literal">True</span> </span><br><span class="line">                project_probs = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">if</span> project_probs:</span><br><span class="line">                    dw_hidden, dd = (x @ <span class="variable language_">self</span>.dyn_w_proj.dw_m).split([<span class="number">2</span>*<span class="number">2</span>*N*(<span class="number">2</span>*I), <span class="number">2</span>*<span class="number">2</span>*N*<span class="number">1</span>], -<span class="number">1</span>)</span><br><span class="line">                    dw_hidden = <span class="variable language_">self</span>.dyn_w_proj.dw_hidden_activation(dw_hidden) </span><br><span class="line">                    dw_hidden = dw_hidden.view(dw_hidden.shape[:<span class="number">2</span>]+(<span class="number">4</span>,-<span class="number">1</span>)) <span class="comment">#B T (4 K) -&gt; B T 4 K  # reshape</span></span><br><span class="line">                    dw = torch.einsum(<span class="string">&#x27;B T C K, C K D -&gt; B T C D&#x27;</span>, dw_hidden, <span class="variable language_">self</span>.dyn_w_proj.qkw_m) <span class="comment"># BT4K,4K(MI)-&gt;BT4(MI)</span></span><br><span class="line">                    shape = (B,T,<span class="number">2</span>*<span class="number">2</span>,-<span class="number">1</span>,N)<span class="comment"># if project_logits else (B,T,2,N,-1)  # BT(pre/post)(q/k)IN</span></span><br><span class="line">                    w1, w2 = dw.view(shape).split(I,-<span class="number">2</span>)</span><br><span class="line">                    w1 = <span class="variable language_">self</span>.dyn_w_proj.dw1_norm(w1) <span class="comment"># BT22IN</span></span><br><span class="line">                    <span class="keyword">if</span> <span class="variable language_">self</span>.use_sw:</span><br><span class="line">                        pre_sw, post_sw = <span class="variable language_">self</span>.dyn_w_proj.sw.unbind(<span class="number">0</span>)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        pre_sw, post_sw = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">                    pre_qw1, pre_kw1, post_qw1, post_kw1 = w1.unbind(<span class="number">2</span>)  <span class="comment"># BT(2&#123;*2&#125;)IN-&gt;[BTIN]*4</span></span><br><span class="line">                    pre_qw2, pre_kw2, post_qw2, post_kw2 = w2.unbind(<span class="number">2</span>)</span><br><span class="line">                    qkdd = F.tanh(dd).squeeze(-<span class="number">1</span>).view(shape[:-<span class="number">2</span>] + (N,)) <span class="comment"># BT(2&#123;*2&#125;)N1-&gt;BT(2&#123;*2&#125;)N</span></span><br><span class="line">                    pre_qdd, pre_kdd, post_qdd, post_kdd = qkdd.unbind(<span class="number">2</span>)  <span class="comment"># BT(2&#123;*2&#125;)N-&gt;[BTN]*4</span></span><br><span class="line"></span><br><span class="line">                y = torch.zeros(B, N, T, D).to(q.device, dtype=torch.float16)</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(T // <span class="variable language_">self</span>.q_chunk_size):</span><br><span class="line">                    start, stop = i * <span class="variable language_">self</span>.q_chunk_size, (i + <span class="number">1</span>) * <span class="variable language_">self</span>.q_chunk_size</span><br><span class="line">                    kv_start = <span class="built_in">max</span>(<span class="number">0</span>, stop - <span class="variable language_">self</span>.q_chunk_size -<span class="variable language_">self</span>.window_size)</span><br><span class="line">                    _q = q[:, :, start : stop, :]</span><br><span class="line">                    _k, _v = k[:, :, kv_start : stop, :], v[:, :, kv_start : stop, :]</span><br><span class="line">                    _atten_mask = mask[:, :, start : stop, kv_start : stop]</span><br><span class="line">                    _pre_proj_dw_args = slice_dw(pre_sw, pre_qw1, pre_qw2, pre_kw1, pre_kw2, pre_qdd, pre_kdd, start, stop, kv_start) \</span><br><span class="line">                        <span class="keyword">if</span> project_logits <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">                    _post_proj_dw_args = slice_dw(post_sw, post_qw1, post_qw2, post_kw1, post_kw2, post_qdd, post_kdd, start,stop,kv_start) \</span><br><span class="line">                        <span class="keyword">if</span> project_probs <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">                    _o = _atten_context(_q, _k, _v, _atten_mask, _pre_proj_dw_args, _post_proj_dw_args)</span><br><span class="line">                    y[:,:,start:stop] = _o</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                y = torch.zeros(B, N, T, D).to(q.device, dtype=torch.float16)</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(T // <span class="variable language_">self</span>.q_chunk_size):</span><br><span class="line">                    start, stop = i * <span class="variable language_">self</span>.q_chunk_size, (i + <span class="number">1</span>) * <span class="variable language_">self</span>.q_chunk_size</span><br><span class="line">                    kv_start = <span class="built_in">max</span>(<span class="number">0</span>, stop - <span class="variable language_">self</span>.q_chunk_size -<span class="variable language_">self</span>.window_size)</span><br><span class="line">                    _q = q[:, :, start : stop, :]</span><br><span class="line">                    _k, _v = k[:, :, kv_start : stop, :], v[:, :, kv_start : stop, :]</span><br><span class="line">                    _atten_mask = mask[:, :, start : stop, kv_start : stop]</span><br><span class="line">                    _pre_proj_dw_args, _post_proj_dw_args = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">                    _o = _atten_context(_q, _k, _v, _atten_mask, _pre_proj_dw_args, _post_proj_dw_args)</span><br><span class="line">                    y[:,:,start:stop] = _o</span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># inference</span></span><br><span class="line">            <span class="keyword">if</span> seqlen == <span class="number">1</span>: <span class="comment"># one-token generation</span></span><br><span class="line">                k_mask = mask <span class="keyword">if</span> <span class="variable language_">self</span>.window_size <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> gen_mask[:, :, :,:<span class="variable language_">self</span>.kv_cache.seq_length]</span><br><span class="line">                <span class="keyword">if</span> fast_infer:</span><br><span class="line">                    y = <span class="variable language_">self</span>._generate_fast(x, input_pos, q, k, v, k_mask)</span><br><span class="line">                <span class="keyword">else</span>: </span><br><span class="line">                    <span class="keyword">assert</span> <span class="keyword">not</span> <span class="variable language_">self</span>.query_wise</span><br><span class="line">                    <span class="comment"># generate dw from hidden_state</span></span><br><span class="line">                    pre_proj_dw_args, post_proj_dw_args, kw_new = <span class="variable language_">self</span>.dyn_w_proj(x, gen_cache=<span class="literal">True</span>)</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># update kvkw cache</span></span><br><span class="line">                    kw_new = kw_new + <span class="variable language_">self</span>.dyn_w_proj.sw <span class="comment"># absorb residual or sw into kw cache</span></span><br><span class="line">                    <span class="keyword">if</span> <span class="variable language_">self</span>.kv_cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                        k, v, kw_out = <span class="variable language_">self</span>.kv_cache.update(input_pos, k, v, kw_val=kw_new) <span class="comment"># BNSD, BNSD, BS2NN</span></span><br><span class="line"></span><br><span class="line">                    logits = q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>) * <span class="variable language_">self</span>.scale_factor</span><br><span class="line">                    <span class="comment"># merge pre_w and apply it</span></span><br><span class="line">                    pre_qw1, pre_qw2, pre_kw1, pre_kw2, pre_qdd, pre_kdd = pre_proj_dw_args</span><br><span class="line">                    pre_qw = torch.einsum(<span class="string">&#x27;BTGIN, BTGIM-&gt;BTNM&#x27;</span>,pre_qw1, pre_qw2)  + torch.diag_embed(pre_qdd.squeeze(<span class="number">2</span>))</span><br><span class="line">                    pre_w = pre_qw + kw_out[:,:,<span class="number">0</span>] <span class="comment"># B1NM, BSNM -&gt; BSNM</span></span><br><span class="line">                    logits = <span class="variable language_">self</span>.dyn_w_proj.pre_proj(logits, proj_w=pre_w.squeeze(<span class="number">1</span>))</span><br><span class="line">  </span><br><span class="line">                    logits = torch.where(k_mask, logits, torch.finfo(torch.float16).<span class="built_in">min</span>)</span><br><span class="line">                    probs = logits.softmax(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># merge post_w and apply it</span></span><br><span class="line">                    post_qw1, post_qw2, post_kw1, post_kw2, post_qdd, post_kdd = post_proj_dw_args</span><br><span class="line">                    post_qw = torch.einsum(<span class="string">&#x27;BTGIN, BTGIM-&gt;BTNM&#x27;</span>, post_qw1, post_qw2) + torch.diag_embed(post_qdd.squeeze(<span class="number">2</span>))</span><br><span class="line">                    post_w = post_qw + kw_out[:,:,<span class="number">1</span>]</span><br><span class="line">                    probs = <span class="variable language_">self</span>.dyn_w_proj.post_proj(probs, proj_w=post_w.squeeze(<span class="number">1</span>)) </span><br><span class="line"></span><br><span class="line">                    y = probs @ v                  </span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># prefill</span></span><br><span class="line">                k_mask = mask[:,:,:,:k.shape[-<span class="number">2</span>]] </span><br><span class="line">                pre_proj_dw_args, post_proj_dw_args,kw_new = <span class="variable language_">self</span>.dyn_w_proj(x, gen_cache=<span class="literal">True</span>)</span><br><span class="line">                kw_new = kw_new + <span class="variable language_">self</span>.dyn_w_proj.sw <span class="comment"># absorb residual or sw into kw cache</span></span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.kv_cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="variable language_">self</span>.kv_cache.update(input_pos, k, v, kw_val=kw_new) <span class="comment"># BNSD, BNSD, BS2NN</span></span><br><span class="line">                logits = q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>) * <span class="variable language_">self</span>.scale_factor </span><br><span class="line">                logits = <span class="variable language_">self</span>.dyn_w_proj.pre_proj(logits, dws=pre_proj_dw_args, query_vec=x, key_vec=x, fast_infer=<span class="literal">True</span>)  <span class="comment"># XD BN1S</span></span><br><span class="line">                logits = torch.where(k_mask, logits, torch.finfo(torch.float16).<span class="built_in">min</span>)</span><br><span class="line">                probs = logits.softmax(-<span class="number">1</span>)</span><br><span class="line">                probs = <span class="variable language_">self</span>.dyn_w_proj.post_proj(probs, dws=post_proj_dw_args, query_vec=x, key_vec=x, fast_infer=<span class="literal">True</span>) <span class="comment"># BN1S</span></span><br><span class="line">                y = probs @ v</span><br><span class="line"></span><br><span class="line">        y = y.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(bsz, seqlen, <span class="variable language_">self</span>.dim)</span><br><span class="line">        y = <span class="variable language_">self</span>.wo(y)</span><br><span class="line">        <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
<p>多头注意力实现，支持动态权重投影和窗口化机制。<br>结合旋转位置嵌入（Rotary Embedding）进行位置编码。<br>提供快速推理模式，通过 _generate_fast 优化生成速度。<br>支持复杂的多阶段窗口机制（如 LG 和 LGLL）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config: DCPythiaConfig</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.w1 = nn.Linear(config.dim, config.intermediate_size, bias=config.use_linear_bias)</span><br><span class="line">        <span class="variable language_">self</span>.w2 = nn.Linear(config.intermediate_size, config.dim, bias=config.use_linear_bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.w2(F.gelu(<span class="variable language_">self</span>.w1(x)))</span><br></pre></td></tr></table></figure>
<p>两层前馈网络，激活函数使用 GELU。<br>处理非线性变换和特征映射。</p>
<p><strong>辅助函数：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_atten_context</span>(<span class="params">query, key, value, atten_mask, pre_proj_dw_args, post_proj_dw_args</span>):</span><br><span class="line">    logits = query @ key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> pre_proj_dw_args <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: logits = _cross_head_proj(logits, *pre_proj_dw_args)</span><br><span class="line">    logits = torch.where(atten_mask, logits, torch.finfo(torch.float16).<span class="built_in">min</span>)</span><br><span class="line">    probs = logits.softmax(-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> post_proj_dw_args <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: probs = _cross_head_proj(probs, *post_proj_dw_args)</span><br><span class="line">    o = probs @ value  <span class="comment"># BNTS,BNSD-&gt;BNTD</span></span><br><span class="line">    <span class="keyword">return</span> o</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_cross_head_proj</span>(<span class="params">inputs, sw, qw1, qw2, kw1, kw2, qdd, kdd, loop_over_dynamic_hd=<span class="literal">False</span></span>):</span><br><span class="line">    out = inputs + torch.einsum(<span class="string">&#x27;BNTS,NM-&gt;BMTS&#x27;</span>, inputs, sw) <span class="keyword">if</span> sw <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> inputs</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>): <span class="comment"># qw1.shape[-2]):</span></span><br><span class="line">        qhidden = (inputs * qw1[..., i, :].transpose(-<span class="number">2</span>, -<span class="number">1</span>).unsqueeze(-<span class="number">1</span>)).<span class="built_in">sum</span>(<span class="number">1</span>)  <span class="comment"># BNTS,(BTN-&gt;BNT-&gt;BNT1)-&gt;BNTS-&gt;BTS</span></span><br><span class="line">        qout = qhidden.unsqueeze(<span class="number">1</span>) * qw2[..., i, :].transpose(-<span class="number">2</span>, -<span class="number">1</span>).unsqueeze(-<span class="number">1</span>) <span class="comment"># (BTS-&gt;B1TS),(BTN-&gt;BNT-&gt;BNT1)-&gt;BNTS</span></span><br><span class="line">        out = out + qout</span><br><span class="line">        khidden = (inputs * kw1[..., i, :].transpose(-<span class="number">2</span>, -<span class="number">1</span>).unsqueeze(-<span class="number">2</span>)).<span class="built_in">sum</span>(<span class="number">1</span>)  <span class="comment"># BNTS,(BSN-&gt;BNS-&gt;BN1S)-&gt;BNTS-&gt;BTS</span></span><br><span class="line">        kout = khidden.unsqueeze(<span class="number">1</span>) * kw2[..., i, :].transpose(-<span class="number">2</span>, -<span class="number">1</span>).unsqueeze(-<span class="number">2</span>) <span class="comment"># (BTS-&gt;B1TS),(BSN-&gt;BNS-&gt;BNS1)-&gt;BNTS</span></span><br><span class="line">        out = out + kout</span><br><span class="line">    qdout = inputs * qdd.transpose(-<span class="number">2</span>, -<span class="number">1</span>).unsqueeze(-<span class="number">1</span>); out = out + qdout  <span class="comment"># BNTS,(BTN-&gt;BNT-&gt;BNT1)-&gt;BNTS</span></span><br><span class="line">    kdout = inputs * kdd.transpose(-<span class="number">2</span>, -<span class="number">1</span>).unsqueeze(-<span class="number">2</span>); out = out + kdout  <span class="comment"># BNTS,(BSN-&gt;BNS-&gt;BN1S)-&gt;BNTS</span></span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">find_multiple</span>(<span class="params">n: <span class="built_in">int</span>, k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">if</span> n % k == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> n</span><br><span class="line">    <span class="keyword">return</span> n + k - (n % k)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_window_mask</span>(<span class="params">t, window_size</span>):</span><br><span class="line">    col_idx = torch.tile(torch.arange(t).unsqueeze(<span class="number">0</span>), [t, <span class="number">1</span>])</span><br><span class="line">    row_idx = torch.tile(torch.arange(t).unsqueeze(<span class="number">1</span>), [<span class="number">1</span>, t])</span><br><span class="line">    bias_mask = (col_idx + window_size &gt;= row_idx).tril().view(t, t)</span><br><span class="line">    <span class="keyword">return</span> bias_mask </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">slice_dw</span>(<span class="params">sw, qw1, qw2, kw1, kw2, qdd, kdd, start, stop, kv_start</span>):</span><br><span class="line">    <span class="keyword">return</span> (sw,</span><br><span class="line">            qw1[:, start : stop] <span class="keyword">if</span> qw1 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">            qw2[:, start : stop] <span class="keyword">if</span> qw2 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">            kw1[:, kv_start : stop] <span class="keyword">if</span> kw1 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">            kw2[:, kv_start : stop] <span class="keyword">if</span> kw2 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">            qdd[:, start : stop] <span class="keyword">if</span> qdd <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">            kdd[:, kv_start : stop] <span class="keyword">if</span> kdd <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">precompute_freqs_cis</span>(<span class="params"></span></span><br><span class="line"><span class="params">    seq_len: <span class="built_in">int</span>, n_elem: <span class="built_in">int</span>, base: <span class="built_in">int</span> = <span class="number">10000</span></span></span><br><span class="line"><span class="params"></span>) -&gt; Tensor:</span><br><span class="line">    freqs = <span class="number">1.0</span> / (base ** (torch.arange(<span class="number">0</span>, n_elem, <span class="number">2</span>)[: (n_elem // <span class="number">2</span>)].<span class="built_in">float</span>() / n_elem))</span><br><span class="line">    t = torch.arange(seq_len, device=freqs.device)</span><br><span class="line">    freqs = torch.outer(t, freqs)</span><br><span class="line">    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)</span><br><span class="line">    cache = torch.stack([freqs_cis.real, freqs_cis.imag], dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> cache.to(dtype=torch.float16)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">unbind</span>(<span class="params">ary, n, dim=<span class="number">0</span></span>):</span><br><span class="line">    <span class="keyword">return</span> [torch.squeeze(a, dim=dim) <span class="keyword">for</span> a <span class="keyword">in</span> torch.split(ary, ary.shape[dim] // n, dim=dim)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apply_rotary_emb</span>(<span class="params">x: Tensor, freqs_cis: Tensor, mode=<span class="string">&#x27;half&#x27;</span></span>) -&gt; Tensor:</span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">&#x27;half&#x27;</span>:</span><br><span class="line">        xshaped = x.<span class="built_in">float</span>().reshape(*x.shape[:-<span class="number">1</span>], <span class="number">2</span>,-<span class="number">1</span>).transpose(-<span class="number">1</span>,-<span class="number">2</span>) </span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">&#x27;alternative&#x27;</span>:</span><br><span class="line">        xshaped = x.<span class="built_in">float</span>().reshape(*x.shape[:-<span class="number">1</span>], -<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    freqs_cis = freqs_cis.view(-<span class="number">1</span>, xshaped.size(<span class="number">1</span>), <span class="number">1</span>, xshaped.size(<span class="number">3</span>), <span class="number">2</span>)</span><br><span class="line">    x_out2 = torch.stack(</span><br><span class="line">        [</span><br><span class="line">            xshaped[..., <span class="number">0</span>] * freqs_cis[..., <span class="number">0</span>] - xshaped[..., <span class="number">1</span>] * freqs_cis[..., <span class="number">1</span>],</span><br><span class="line">            xshaped[..., <span class="number">1</span>] * freqs_cis[..., <span class="number">0</span>] + xshaped[..., <span class="number">0</span>] * freqs_cis[..., <span class="number">1</span>],</span><br><span class="line">        ],</span><br><span class="line">        -<span class="number">1</span>,</span><br><span class="line">    )</span><br><span class="line">    x_out2 = x_out2.flatten(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> x_out2.type_as(x)</span><br></pre></td></tr></table></figure>
<p>提供位置编码 (precompute_freqs_cis)、窗口掩码生成 (make_window_mask)、嵌入旋转 (apply_rotary_emb) 和张量拆分 (unbind) 等工具函数。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://SEER666.github.io">SEER</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://seer666.github.io/2024/11/28/20241128/">https://seer666.github.io/2024/11/28/20241128/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://SEER666.github.io" target="_blank">SEER's Study</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E4%BB%A3%E7%A0%81/">代码</a></div><div class="post-share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/12/10/20241210/" title="第1章 操作系统概论"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">第1章 操作系统概论</div></div><div class="info-2"><div class="info-item-1">   window.MathJax = {     tex: {       inlineMath: [['$', '$'], ['\\(', '\\)']],         displayMath: [['$$', '$$'], ['\\[', '\\]']]       }   };    1.1 操作系统的定义计算机系统的组成：应用程序 -&gt; 实用程序 -&gt; 操作系统 -&gt; 硬件 接口：外层的软件必须以事先约定的方式，调用内层软件或硬件提供的服务。 操作系统的定义： 是由一系列程序模块组成的一个大型的系统管理程序；依据各种管理和调度策略，对计算机的软、硬件资源进行统一的管理和调度，合理地组织计算机的工作流程，以提高资源利用率。 1.2...</div></div></div></a><a class="pagination-related" href="/2024/11/24/2024112402/" title="Transformer示例代码解读"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Transformer示例代码解读</div></div><div class="info-2"><div class="info-item-1">   window.MathJax = {     tex: {       inlineMath: [['$', '$'], ['\\(', '\\)']],         displayMath: [['$$', '$$'], ['\\[', '\\]']]       }  ...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/11/24/2024112402/" title="Transformer示例代码解读"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="info-item-2">Transformer示例代码解读</div></div><div class="info-2"><div class="info-item-1">   window.MathJax = {     tex: {       inlineMath: [['$', '$'], ['\\(', '\\)']],         displayMath: [['$$', '$$'], ['\\[', '\\]']]       }  ...</div></div></div></a><a class="pagination-related" href="/2024/11/24/2024112401/" title="Transformer文档阅读"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="info-item-2">Transformer文档阅读</div></div><div class="info-2"><div class="info-item-1">   window.MathJax = {     tex: {       inlineMath: [['$', '$'], ['\\(', '\\)']],         displayMath: [['$$', '$$'], ['\\[', '\\]']]       }   };    1.torch.nn.Transformerclasstorch.nn.Transformer(d_model&#x3D;512, nhead&#x3D;8, num_encoder_layers&#x3D;6, num_decoder_layers&#x3D;6, dim_feedforward&#x3D;2048, dropout&#x3D;0.1, activation&#x3D;, custom_encoder&#x3D;None, custom_decoder&#x3D;None, layer_norm_eps&#x3D;1e-05, batch_first&#x3D;False, norm_first&#x3D;False, bias&#x3D;True,...</div></div></div></a><a class="pagination-related" href="/2025/01/09/20250109/" title="CSAPP考前题"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-09</div><div class="info-item-2">CSAPP考前题</div></div><div class="info-2"><div class="info-item-1">   window.MathJax = {     tex: {       inlineMath: [['$', '$'], ['\\(', '\\)']],         displayMath: [['$$', '$$'], ['\\[', '\\]']]       }   };    一、系统漫游线上测试1.以下说法正确的是（   ）。a.处理器顺序执行机器指令。b.主存储器包括寄存器。c.总线系统只用来传输数据，不传输指令。d.中央处理器（CPU）是特定指令集架构下的执行单元。 答案：D解析：  a项，目前多半是指令级并行、流水线技术，未必顺序执行； b项，著储存器与寄存器是两种不同的存储类型。寄存器是CPU内部的高速存储单元，主要用于临时存储数据和指令，而主存储器是外部于CPU的，通常用来存储程序和数据。 c项，总线系统可以传输指令。  2.可执行目标程序是（   ）。a.在目标机运行的汇编语言程序。b.是机器指令被按照固定格式打包的二进制文件。c.由编译器产生的汇编程序。d.在目标机运行的高级语言程序。 答案：B 3.SHELL的功能是（  ...</div></div></div></a></div></div><!-- 添加 Valine 评论系统的 HTML 容器--><!-- Valine 评论系统--><div id="vcomments"></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="https://avatars.githubusercontent.com/u/126209991?v=4" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">SEER</div><div class="author-info-description">Record SEER's learning content.</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">37</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/SEER666"><i class="fab fa-github"></i><span>关注我</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">我的博客已全面升级</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-DCFormer"><span class="toc-number">1.</span> <span class="toc-text">1 DCFormer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-config-json"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 config.json</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-configuration-dcformer-py"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 configuration_dcformer.py</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-generation-demo-py"><span class="toc-number">1.3.</span> <span class="toc-text">1.3 generation_demo.py</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-maxtext2torch-py"><span class="toc-number">1.4.</span> <span class="toc-text">1.4 maxtext2torch.py</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-modeling-dcformer-py"><span class="toc-number">1.5.</span> <span class="toc-text">1.5 modeling_dcformer.py</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-1-KVKWCache"><span class="toc-number">1.5.1.</span> <span class="toc-text">1.5.1 KVKWCache</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-2-DCFormer"><span class="toc-number">1.5.2.</span> <span class="toc-text">1.5.2 DCFormer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-3-DCFormerBlock"><span class="toc-number">1.5.3.</span> <span class="toc-text">1.5.3 DCFormerBlock</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-4-DCMHAttention"><span class="toc-number">1.5.4.</span> <span class="toc-text">1.5.4 DCMHAttention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-5-DynamicWeightProjection"><span class="toc-number">1.5.5.</span> <span class="toc-text">1.5.5 DynamicWeightProjection</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-6-FeedForward"><span class="toc-number">1.5.6.</span> <span class="toc-text">1.5.6 FeedForward</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-7-RMSNorm"><span class="toc-number">1.5.7.</span> <span class="toc-text">1.5.7 RMSNorm</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-8-%E8%BE%85%E5%8A%A9%E5%87%BD%E6%95%B0"><span class="toc-number">1.5.8.</span> <span class="toc-text">1.5.8 辅助函数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-DCpythia"><span class="toc-number">2.</span> <span class="toc-text">2 DCpythia</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#config-json"><span class="toc-number">2.1.</span> <span class="toc-text">config.json</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#configuration-dcythia-py"><span class="toc-number">2.2.</span> <span class="toc-text">configuration_dcythia.py</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#generation-demo-py"><span class="toc-number">2.3.</span> <span class="toc-text">generation_demo.py</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#modeling-dcpythia-py"><span class="toc-number">2.4.</span> <span class="toc-text">modeling_dcpythia.py</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/01/09/20250109/" title="CSAPP考前题"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CSAPP考前题"/></a><div class="content"><a class="title" href="/2025/01/09/20250109/" title="CSAPP考前题">CSAPP考前题</a><time datetime="2025-01-09T06:30:00.000Z" title="发表于 2025-01-09 14:30:00">2025-01-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/04/20241105%20-%20%E5%89%AF%E6%9C%AC/" title="2025-CSAPP"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2025-CSAPP"/></a><div class="content"><a class="title" href="/2025/01/04/20241105%20-%20%E5%89%AF%E6%9C%AC/" title="2025-CSAPP">2025-CSAPP</a><time datetime="2025-01-04T11:00:00.000Z" title="发表于 2025-01-04 19:00:00">2025-01-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/04/20250106/" title="2025-CSAPP"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2025-CSAPP"/></a><div class="content"><a class="title" href="/2025/01/04/20250106/" title="2025-CSAPP">2025-CSAPP</a><time datetime="2025-01-04T11:00:00.000Z" title="发表于 2025-01-04 19:00:00">2025-01-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/24/20241224/" title="汇编语言知识点复习"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="汇编语言知识点复习"/></a><div class="content"><a class="title" href="/2024/12/24/20241224/" title="汇编语言知识点复习">汇编语言知识点复习</a><time datetime="2024-12-24T13:30:00.000Z" title="发表于 2024-12-24 21:30:00">2024-12-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/22/20241222/" title="第5章 文件系统（概要）"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="第5章 文件系统（概要）"/></a><div class="content"><a class="title" href="/2024/12/22/20241222/" title="第5章 文件系统（概要）">第5章 文件系统（概要）</a><time datetime="2024-12-22T13:30:00.000Z" title="发表于 2024-12-22 21:30:00">2024-12-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By SEER</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.1.0"></script><script src="/js/main.js?v=5.1.0"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (false) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@11.3.0/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/dist/click-show-text.min.js" data-mobile="true" data-text="我,太,想,进,步,了" data-fontsize="15px" data-random="false" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script><script>new Valine({
  el: '#vcomments',  // 挂载评论的 HTML 元素
  appId: 'X7VMJZiKLlND8y3EXDtaqPI5-gzGzoHsz',  // LeanCloud 的 App ID
  appKey: 'zFn5LKNiWC2vPb3y0RsW1nMK',  // LeanCloud 的 App Key
  path: window.location.pathname,  // 使用文章的路径作为评论的唯一标识
  placeholder: '留下你的评论吧...'  // 输入框的占位文本
})</script></body></html>